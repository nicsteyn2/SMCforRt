[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SMC and epidemic renewal models",
    "section": "",
    "text": "Welcome\nRenewal models are popular in statistical epidemiology for their use as a semi-mechanistic model of disease transmission. We demonstrate how sequential Monte Carlo (SMC) methods can be used to perform inference on and generate projections with these models.\nWith these methods one can:\nThis approach:\nRenewal models underlie many of the most popular methods for reproduction number estimation, including EpiEstim (Cori et al. 2013) and EpiNow2 (Abbott et al. 2020). They have also been used to model elimination probabilities (Parag, Cowling, and Donnelly 2021), estimate the effect of non-pharmaceutical interventions (Flaxman et al. 2020), and produce forecasts (Banholzer et al. 2023), for example.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "SMC and epidemic renewal models",
    "section": "",
    "text": "Estimate \\(R_t\\) from imperfect data, or from multiple sources of data\nProduce well-calibrated short-term projections\nEstimate abrupt change-points in disease transmission\n… and so much more\n\n\n\nIs simple, intuitive, and flexible\nProduces valid credible intervals on quantities of interest\nCan simultaneously account for reporting biases, aggregated/missing data, imported cases, multiple data sources, and more.\nUses entirely standalone code (i.e. no external packages or software are required)\nCan handle a mixture of continuous and discrete variables (where Stan requires only continuous variablese)\n\n\n\n\nReported cases for the first 100 days of the COVID-19 pandemic in Aotearoa New Zealand Ministry of Health NZ (2024). We will develop methods to estimate the reproduction number from these data.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#structure-of-this-website",
    "href": "index.html#structure-of-this-website",
    "title": "SMC and epidemic renewal models",
    "section": "Structure of this website",
    "text": "Structure of this website\nThis website is made up of a collection of Jupyter-style notebooks. All code is implemented in Julia, a programming language with the speed of C++ and simplicity of R and Python. The notebooks are organised into chapters:\n\nIntroduction: We introduce key concepts such as the renewal model and hidden-state models. Those already familiar with these concepts can safely skip these pages.\nSequential Monte Carlo: We introduce specific SMC methods. This includes defining the bootstrap filter and particle marginal Metropolis Hastings.\nModels: A collection of epidemic models accounting for various statistical artefacts. The focus in this chapter is on \\(R_t\\) estimation. \nEvaluation: We introduce key metrics for model evaluation, including scoring rules, KL-divergence, and TBC. \nOther methods: We compare our methods to popular alternatives in the literature. Where possible, Julia implementations are provided for these.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#using-julia",
    "href": "index.html#using-julia",
    "title": "SMC and epidemic renewal models",
    "section": "Using Julia",
    "text": "Using Julia\nWe recognise that Julia is not widely used in epidemiology. While we would love to implement our methods in R, they would be extremely slow without a C++ backend. Instead, our methods are all natively implemented in Julia and require no external software - making the entire process much simpler!\nGetting started with Julia is easy (see below) and our examples are written for those with no prior knowledge of the language.\n\n\n\n\n\n\nJulia\n\n\n\nThroughout the examples, we use blocks like this one to clarify any notation that may be confusing to users new to Julia.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "SMC and epidemic renewal models",
    "section": "Getting started",
    "text": "Getting started\nWe do not use any external dependencies, so you can run all code in this repository with a standard Julia installation. To get started, all you need to do is:\n\nInstall Julia from julialang.org\nClone this repository\nStart Julia in your terminal and run:\n\nusing Pkg\nPkg.add(\"IJulia\")\nusing IJulia\nnotebook()\nThis will open a Jupyter notebook in your browser. Navigate to the GitHub repository and open main.ipynb to get started.\nAlternatively, you can open the cloned repository in VS Code (or your preferred IDE) and run the notebooks this way. If using VS Code, we recommend installing the Julia extension.\n\nGitHub layout\nThe files and folders you should care about:\n\n/notebooks/: Contains the main tutorial notebooks\n/src/: Contains important source code\n/data/: Contains all example data used in the tutorial\n\nYou can ignore (but feel free to explore):\n\n/paper/: Scripts and outputs associated with the accompanying paper\n/docs/: Contains the rendered tutorial\n/site/: Contains the quarto website files (these are rendered to /docs/)\n/assets/: Contains images and other assets used in the tutorials and readme files\nand files like .gitignore and .nojekyll which are for repo management\n\n\n\n\n\nAbbott, Sam, Joel Hellewell, Robin N. Thompson, Katharine Sherratt, Hamish P. Gibbs, Nikos I. Bosse, James D. Munday, et al. 2020. “Estimating the Time-Varying Reproduction Number of SARS-CoV-2 Using National and Subnational Case Counts.” Wellcome Open Research 5 (December): 112. https://doi.org/10.12688/wellcomeopenres.16006.2.\n\n\nBanholzer, Nicolas, Thomas Mellan, H. Juliette T. Unwin, Stefan Feuerriegel, Swapnil Mishra, and Samir Bhatt. 2023. “A Comparison of Short-Term Probabilistic Forecasts for the Incidence of COVID-19 Using Mechanistic and Statistical Time Series Models.” arXiv. https://doi.org/10.48550/arXiv.2305.00933.\n\n\nCori, Anne, Neil M. Ferguson, Christophe Fraser, and Simon Cauchemez. 2013. “A New Framework and Software to Estimate Time-Varying Reproduction Numbers During Epidemics.” American Journal of Epidemiology 178 (9): 1505–12. https://doi.org/10.1093/aje/kwt133.\n\n\nFlaxman, Seth, Swapnil Mishra, Axel Gandy, H. Juliette T. Unwin, Thomas A. Mellan, Helen Coupland, Charles Whittaker, et al. 2020. “Estimating the Effects of Non-Pharmaceutical Interventions on COVID-19 in Europe.” Nature 584 (7820): 257–61. https://doi.org/10.1038/s41586-020-2405-7.\n\n\nMinistry of Health NZ. 2024. “New Zealand COVID-19 Data.”\n\n\nParag, Kris V., Benjamin J. Cowling, and Christl A. Donnelly. 2021. “Deciphering Early-Warning Signals of SARS-CoV-2 Elimination and Resurgence from Limited Data at Multiple Scales.” Journal of The Royal Society Interface 18 (185): 20210569. https://doi.org/10.1098/rsif.2021.0569.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  The renewal model",
    "section": "",
    "text": "1.1 Simulating the renewal model\nThe renewal model is a simple model of infectious disease transmission. It relates past cases to current cases through a serial interval and reproduction number. It is typically written:\n\\[\nE[C_t] = R_t \\sum_{u=1}^{u_{max}} C_{t-u} \\omega_u\n\\tag{1.1}\\]\nwhere:\nWe also need to specify a distribution for \\(C_t\\). The canonical choice is the Poisson renewal model:\n\\[\nC_t|R_t, C_{1:t-1} \\sim \\text{Poisson}\\left(R_t \\sum_{u=1}^{u_{max}} C_{t-u} \\omega_u \\right)\n\\tag{1.2}\\]\nFinally, we often denote the summation in the renewal model using:\n\\[\n\\Lambda_t^c = \\sum_{u=1}^{u_{max}} C_{t-u} \\omega_u\n\\tag{1.3}\\]\nwhere \\(\\Lambda_t^c\\) is called the force-of-infection at time \\(t\\). The superscript \\(c\\) denotes that \\(\\Lambda_t^c\\) is calculated using past reported cases, to differentiate it from \\(\\Lambda_t\\), which we use when modelling infections (see Section 1.4).\nTo understand how the renewal model works, let’s start by simulating \\(T = 100\\) days of reported cases from it. To do this, we need three components:\nT = 100\nC = zeros(T)\nC[1] = 50 # Specify C_1 = 50\nR = 1.0 .+ 0.5 * sin.((2*π/50) .* (1:T))\nusing Distributions\n\nω = pdf.(Gamma(2.36, 2.74), 1:T)\nω = ω/sum(ω) # Ensure it is normalised!\nPlotting our chosen \\(R_t\\) and serial interval:\nCode\n# Visualise Rt and the serial interval\nusing Plots, Measures\nplotR = plot(R, label=false, xlabel=\"Time (days)\", ylabel=\"Reproduction number\", color=:darkgreen, linewidth=3)\nplotω = bar(1:21, ω[1:21], label=false, xlabel=\"Day\", ylabel=\"Serial interval probability\", color=:darkorange)\ndisplay(plot(plotR, plotω, layout=(1,2), size=(800,300), margins=3mm))\n\n\n\n\n\n\n\n\nFigure 1.2: Assumed time-varying reproduction number and serial intervals.\nNow we are ready to simulate from the renewal model. We do this by iteratively sampling a new \\(C_t\\) and calculating the new force-of-infection term:\nfor tt = 2:T\n\n    # Calculate the force-of-infection\n    Λ = sum(C[tt-1:-1:1] .* ω[1:tt-1])/sum(ω[1:tt-1])\n\n    # And sample from the appropriate Poisson distribution\n    C[tt] = rand(Poisson(R[tt] * Λ))\n\nend\nFinally letting us plot our simulated cases:\nCode\ndisplay(bar(C, label=false, xlabel=\"Time (days)\", ylabel=\"Simulated cases\", size=(800,300), margins=3mm, color=:darkblue))\n\n\n\n\n\n\n\n\nFigure 1.3: Simulated epidemic from the renewal model using our chosen values of Rt and ω.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The renewal model</span>"
    ]
  },
  {
    "objectID": "introduction.html#simulating-the-renewal-model",
    "href": "introduction.html#simulating-the-renewal-model",
    "title": "1  The renewal model",
    "section": "",
    "text": "Initial cases \\(C_1\\). Let’s start with \\(C_1 = 50\\).\n\n\n\nThe reproduction number over time. We will use a sin-curve alternating between \\(R_t = 1.5\\) and \\(R_t = 0.5\\) with a period of 50 days for this example:\n\n\n\nA serial interval. We use a discretised Gamma(2.36, 2.74)1 distribution:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia\n\n\n\nThese codeblocks are written in the programming language Julia, which should (mostly) make sense to those familiar with R/Python/MATLAB. One key difference between these languages and Julia is broadcasting.\nBroadcasting allows functions to be called elementwise. For example, when defining R above, we write:\nR = 1.0 .+ 0.5 * sin.(...)\nAs 1.0 is a scalar and 0.5 * sin.(...) is a vector, the . before the + tells Julia to add 1 to each element of 0.5 * sin.(...). Similarly, the . in sin.(x) tells Julia to apply the sine function to each element of x. Other languages may handle this automatically in some cases, but by being explicit about element-wise operations, Julia avoids ambiguity.\nBroadcasting can also be more memory-efficient in certain situations. It also works automatically with user-defined functions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The renewal model</span>"
    ]
  },
  {
    "objectID": "introduction.html#sec-intro-Rtestimation",
    "href": "introduction.html#sec-intro-Rtestimation",
    "title": "1  The renewal model",
    "section": "1.2 Estimating \\(R_t\\)",
    "text": "1.2 Estimating \\(R_t\\)\n\\(R_t\\) is a crucial component in the renewal model thus making the renewal model a natural choice for \\(R_t\\) estimation. In fact, even if your goal is not to estimate \\(R_t\\), it is helpful to consider this briefly.\nIf \\(C_t\\) is large and the model accurately reflects reality, we can use Equation 1.2 to estimate \\(R_t\\) directly. In the Bayesian setting, a prior distribution is placed on \\(R_t\\) and standard methods are used to find \\(P(R_t | C_{1:t})\\). However, often \\(C_t\\) is small and the data are subject to noise and bias. Estimates from the naive method are thus highly variable.\n\n1.2.1 Example\nLet’s pretend we don’t know \\(R_t\\) and want to estimate it from the simulated data. Like Cori et al. (2013), we will use a Gamma prior distribution for \\(R_t\\) with shape \\(a_0 = 1\\) and rate \\(b_0 = 0.2\\). As our likelihood is a Poisson distribution, we have a conjugate prior-likelihood, and thus our posterior distribution for \\(R_t\\) is:\n\\[\nR_t | C_{1:t} \\sim \\text{Gamma}\\left(a_0 + C_t, b_0 + \\sum_{u=1}^{u_{max}} C_{t-u} \\omega_u \\right)\n\\tag{1.4}\\]\n\n(MeanRt, LowerRt, UpperRt) = (zeros(T), zeros(T), zeros(T)) # Pre-allocate results vectors\n(a0, b0) = (1, 1/5) # Set prior parameters\n\nfor tt = 2:T\n\n    # Find the posterior distribution on day t\n    a = a0 + C[tt]\n    b = b0 + sum(C[tt-1:-1:1] .* ω[1:tt-1])/sum(ω[1:tt-1])\n    PosteriorDist = Gamma(a, 1/b)\n\n    # Save the results\n    MeanRt[tt] = mean(PosteriorDist)\n    LowerRt[tt] = quantile(PosteriorDist, 0.025)\n    UpperRt[tt] = quantile(PosteriorDist, 0.975)\n\nend\n\nWe will compare our estimates with those from a popular model, EpiEstim, which smooths the data by assuming \\(R_t\\) is fixed over a \\(\\tau\\)-day (typically \\(\\tau = 7\\)) trailing window:\n\ninclude(\"../src/RtEstimators.jl\")\nEpiEstimPosterior = EpiEstim(7, ω, C; a0=a0, b0=b0)\n(EpiEstMean, EpiEstLower, EpiEstUpper) = (mean.(EpiEstimPosterior), quantile.(EpiEstimPosterior, 0.025), quantile.(EpiEstimPosterior, 0.975))\n\nFinally, we are ready to plot our results:\n\n\nCode\nplot(xlabel=\"Time (days)\", ylabel=\"Reproduction number\", size=(800,350), left_margin=3mm, bottom_margin=3mm)\nplot!(2:T, MeanRt[2:T], ribbon=(MeanRt[2:T]-LowerRt[2:T], UpperRt[2:T]-MeanRt[2:T]), fillalpha=0.4, label=\"Unsmoothed posterior\")\nplot!(2:T, EpiEstMean[2:T], ribbon=(EpiEstMean[2:T]-EpiEstLower[2:T], EpiEstUpper[2:T]-EpiEstMean[2:T]), fillalpha=0.4, label=\"EpiEstim (smoothed) posterior\")\nplot!(1:T, R, label=\"True Rt\", color=:black)\n\n\n\n\n\n\n\n\nFigure 1.4: Estimates of Rt from the basic (unsmoothed) renewal model (blue) and from a smoothed model (EpiEstim, orange).\n\n\n\n\n\n\n\n1.2.2 The necessity and dangers of smoothing\nFigure 1.4 highlights both the necessity and dangers of smoothing. Our independent daily estimates (blue) are highly variable and the credible intervals are wide. By using smoothed estimates (orange), we reduce this variance and produce much more confident results. However, our results strongly depend on these smoothing assumptions. In the example above, we can clearly see that the credible intervals produced by EpiEstim often do not include the true value of \\(R_t\\)!\nSmoothing works by allowing data from multiple days to inform point-estimates. A variety of approaches have been developed, discussed at length in Appendix C. In fact, many popular renewal-model based estimators of \\(R_t\\) differ only in their choice of smoothing method!\nEpidemic renewal models are usually smoothed by placing assumptions on the dynamics of \\(R_t\\). Examples include assuming \\(R_t\\) is fixed over trailing windows (Cori et al. 2013), modelling it with splines (Azmon, Faes, and Hens 2014) or Gaussian processes (Abbott et al. 2020), or assuming it follows a random walk (Parag 2021). Piecewise-constant models, where \\(R_t\\) is assumed to be fixed over different time-windows, are also examples of smoothing (Creswell et al. 2023).\nSo far we have only considered process noise in the epidemic, but epidemic data are often subject to observation noise, a secondary reason why smoothing is so important.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The renewal model</span>"
    ]
  },
  {
    "objectID": "introduction.html#the-serial-interval",
    "href": "introduction.html#the-serial-interval",
    "title": "1  The renewal model",
    "section": "1.3 The serial interval",
    "text": "1.3 The serial interval\nThe other key component in the renewal model is the serial interval \\(\\omega\\). This parameter is typically not identifiable from reported case data (at least at the same time as \\(R_t\\)), so it often receives less attention. When fitting renewal models, researchers usually use estimates of the serial interval from other data.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The renewal model</span>"
    ]
  },
  {
    "objectID": "introduction.html#sec-intro-casesvsinfections",
    "href": "introduction.html#sec-intro-casesvsinfections",
    "title": "1  The renewal model",
    "section": "1.4 Reported cases vs infections",
    "text": "1.4 Reported cases vs infections\nThe simple renewal model in Equation 1.2 assumes that old reported cases directly cause new reported cases. This leaves little room for observation noise. Instead, we can assume that old (but typically unobserved) infections cause new infections, writing:\n\\[\nI_t|R_t, I_{1:t-1} \\sim \\text{Poisson}\\left(R_t \\Lambda_t\\right)\n\\tag{1.5}\\]\nThe force-of-infection becomes:\n\\[\n\\Lambda_t = \\sum_{u=1}^{u_{max}} I_{t-u} g_u\n\\tag{1.6}\\]\nWhere we have replaced the serial interval \\(\\omega_u\\) with a generation time distribution \\(g_u\\), reflecting that we are modelling the delay between infection events instead of reporting events.\nExplicitly modelling infections allows us to define an observation distribution:\n\\[\nP(C_t | I_{1:t})\n\\tag{1.7}\\]\nwhich explicitly links our hidden (a.k.a latent) infections to our reported cases. A plethora of methods exist that can estimate \\(I_{1:T}\\) given \\(C_{1:T}\\).\nSeparating case reporting from transmission allows us to model process noise and observation noise separately. This is one of the key advantages provided by our SMC methods.\n\n\n\n\nAbbott, Sam, Joel Hellewell, Robin N. Thompson, Katharine Sherratt, Hamish P. Gibbs, Nikos I. Bosse, James D. Munday, et al. 2020. “Estimating the Time-Varying Reproduction Number of SARS-CoV-2 Using National and Subnational Case Counts.” Wellcome Open Research 5 (December): 112. https://doi.org/10.12688/wellcomeopenres.16006.2.\n\n\nAzmon, Amin, Christel Faes, and Niel Hens. 2014. “On the Estimation of the Reproduction Number Based on Misreported Epidemic Data.” Statistics in Medicine 33 (7): 1176–92. https://doi.org/10.1002/sim.6015.\n\n\nCori, Anne, Neil M. Ferguson, Christophe Fraser, and Simon Cauchemez. 2013. “A New Framework and Software to Estimate Time-Varying Reproduction Numbers During Epidemics.” American Journal of Epidemiology 178 (9): 1505–12. https://doi.org/10.1093/aje/kwt133.\n\n\nCreswell, Richard, Martin Robinson, David Gavaghan, Kris V. Parag, Chon Lok Lei, and Ben Lambert. 2023. “A Bayesian Nonparametric Method for Detecting Rapid Changes in Disease Transmission.” Journal of Theoretical Biology 558 (February): 111351. https://doi.org/10.1016/j.jtbi.2022.111351.\n\n\nFerguson, N, D Laydon, G Nedjati Gilani, N Imai, K Ainslie, M Baguelin, S Bhatia, et al. 2020. “Report 9: Impact of Non-Pharmaceutical Interventions (NPIs) to Reduce COVID19 Mortality and Healthcare Demand.” Imperial College London. https://doi.org/10.25561/77482.\n\n\nParag, Kris V. 2021. “Improved Estimation of Time-Varying Reproduction Numbers at Low Case Incidence and Between Epidemic Waves.” PLOS Computational Biology 17 (9): e1009347. https://doi.org/10.1371/journal.pcbi.1009347.\n\n\nParag, Kris V., Benjamin J. Cowling, and Christl A. Donnelly. 2021. “Deciphering Early-Warning Signals of SARS-CoV-2 Elimination and Resurgence from Limited Data at Multiple Scales.” Journal of The Royal Society Interface 18 (185): 20210569. https://doi.org/10.1098/rsif.2021.0569.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The renewal model</span>"
    ]
  },
  {
    "objectID": "introduction.html#footnotes",
    "href": "introduction.html#footnotes",
    "title": "1  The renewal model",
    "section": "",
    "text": "This is a popular serial interval used in early COVID-19 models (Parag, Cowling, and Donnelly 2021; Ferguson et al. 2020).↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The renewal model</span>"
    ]
  },
  {
    "objectID": "intro_Rtestimation.html",
    "href": "intro_Rtestimation.html",
    "title": "2  Background of \\(R_t\\) estimation",
    "section": "",
    "text": "There exists a wide range of approaches to reproduction number estimation spanning different statistical philosophies, model structures, estimation methods, data sources, and computational techniques. We provide a brief overview of these here.\n\n\n\n\n\n\nNote\n\n\n\nThis section provides general context and can be safely skipped if you just want to fit a model! We first outline our method for \\(R_t\\) estimation in Section 1.2.\n\n\nWith perfect knowledge we could calculate \\(R_t\\) by counting the number of secondary cases generated by each primary case. In practice this is impossible, so we must use data to estimate this quantity. Cori and Kucharski (2024) highlight that \\(R_t\\) can be estimated by multiplying estimates of contact rates and transmission probabilities, or empirically from contact tracing data. However, in practice, we typically use time-series data such as reported case counts to estimate \\(R_t\\).\nMost \\(R_t\\) estimators are statistical: they seek to estimate \\(R_t\\) and the associated uncertainty using a statistical model (Steyn and Parag 2024). Purely mathematical methods, i.e. those that do not provide uncertainty estimates, also exist (typically coupling a mathematical model of disease transmission with an optimisation routine to fit the model to data). Robust uncertainty quantification is a key focus of this work, thus our methods fall into the statistical category.\nStatistical estimators of \\(R_t\\) can generally be categorised as either Bayesian (where \\(R_t\\) and other parameters are treated as random variables with associated prior distributions) or frequentist (where \\(R_t\\) is treated as a fixed quantity). Bayesian methods are currently more popular than frequentist methods, as they provide a natural way to quantify uncertainty in \\(R_t\\) estimates (whereas frequentist methods typically rely upon bootstrapping or large-sample arguments), and highly effective simulation-based methods exist for Bayesian methods. SMC (in our context) is an example of a Bayesian simulation-based method.\nBayesian estimators are constructed by assuming a data generating process (inducing a model likelihood) and a prior distribution over the parameters of this process. The likelihood and prior distribution are together termed the model. Alongside the model, a method is required to find the posterior distribution, which can be found analytically (in the case of a conjugate prior and likelihood) or computationally (such as MCMC and ABC). While some methods are better suited to certain models, we encourage a clear separation of the two, a distinction that is often blurred in the literature.\nA multitude of data generating processes have been proposed for \\(R_t\\) estimation, including renewal models, compartmental differential equation models, network models, and agent-based models. Renewal models target \\(R_t\\) directly, requiring the fewest assumptions about the underlying disease dynamics.\nWhile the renewal model can be employed as is, daily data are typically noisy and incomplete, so some smoothing assumptions must be made. EpiEstim uses a trailing window over which \\(R_t\\) is assumed to be fixed (Cori et al. 2013), EpiFilter assumes \\(R_t\\) follows a Gaussian random walk (Parag 2021), while EpiNow2 assumes \\(R_t\\) follows a Gaussian process (Abbott et al. 2020). In addition to the renewal model itself, the smoothing method also forms part of the statistical model.\nFinally, once the model has been set, a method must be chosen to estimate \\(R_t\\) and other parameters. For example, EpiEstim uses a Gamma-prior distribution for \\(R_t\\) that is conjugate to the likelihood function, thus producing an analytical posterior distribution. EpiFilter uses a grid-based approximation to the Bayesian filtering and smoothing equations, while EpiNow2 fits an approximation to the assumed Gaussian process using MCMC methods. We use a sequential Monte Carlo approach.\n\n\n\n\nAbbott, Sam, Joel Hellewell, Robin N. Thompson, Katharine Sherratt, Hamish P. Gibbs, Nikos I. Bosse, James D. Munday, et al. 2020. “Estimating the Time-Varying Reproduction Number of SARS-CoV-2 Using National and Subnational Case Counts.” Wellcome Open Research 5 (December): 112. https://doi.org/10.12688/wellcomeopenres.16006.2.\n\n\nCori, Anne, Neil M. Ferguson, Christophe Fraser, and Simon Cauchemez. 2013. “A New Framework and Software to Estimate Time-Varying Reproduction Numbers During Epidemics.” American Journal of Epidemiology 178 (9): 1505–12. https://doi.org/10.1093/aje/kwt133.\n\n\nCori, Anne, and Adam Kucharski. 2024. “Inference of Epidemic Dynamics in the COVID-19 Era and Beyond.” Epidemics 48 (September): 100784. https://doi.org/10.1016/j.epidem.2024.100784.\n\n\nParag, Kris V. 2021. “Improved Estimation of Time-Varying Reproduction Numbers at Low Case Incidence and Between Epidemic Waves.” PLOS Computational Biology 17 (9): e1009347. https://doi.org/10.1371/journal.pcbi.1009347.\n\n\nSteyn, Nicholas, and Kris V. Parag. 2024. “Robust Uncertainty Quantification in Popular Estimators of the Instantaneous Reproduction Number.” medRxiv. https://doi.org/10.1101/2024.10.22.24315918.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Background of $R_t$ estimation</span>"
    ]
  },
  {
    "objectID": "intro-hiddenstatemodels.html",
    "href": "intro-hiddenstatemodels.html",
    "title": "3  Hidden-state models",
    "section": "",
    "text": "3.1 Definition: hidden-state model\nSequential Monte Carlo (SMC) methods are a class of methods used to solve hidden-state models1.\nIt is helpful to define some general notation:\nDepending on your goal, you may only care about some of these terms, and might want to marginalise out the terms you’re not interested in. Nevertheless, it is important to consider all three.\nIf your goal is forecasting, then future observed data \\(Y_{t+k}\\) are your quantity of interest, and you only care about \\(X_t\\) and \\(\\theta\\) to the extent that they allow you to estimate \\(Y_{t+k}\\).\nIf your goal is reproduction number estimation (or the estimation of any hidden-state), then \\(\\theta\\) is often a nuisance parameter which you only care about to the extent that it allows you to estimate \\(R_t\\).\nIf your goal is to learn about \\(\\theta\\), which may represent the effect of a non-pharmaceutical intervention, for example, then \\(\\theta\\) is likely your quantity of interest, and you only care about \\(X_t\\) to the extent that it allows you to estimate this effect.\nA hidden-state model has two components:\nA state-space transition distribution:\n\\[\nP(X_t | X_{1:t-1}, \\theta)\n\\tag{3.1}\\]\nwhich dictates how the hidden-states vary over time.\nAn observation distribution: \\[\nP(y_t | X_{1:t-1}, y_{1:t-1}, \\theta)\n\\tag{3.2}\\]\nwhich relates the observed data to the hidden-states.\nThese two distributions wholly define the model.\nThis structure makes it clear why hidden-state models are so popular in epidemiology. The underlying epidemic is often unobserved (thus represented by the state-space model), while reported cases (or other data) are generated through some observation process.\nWe note that we make no Markov-type assumption, although Markovian models (a.k.a. Hidden Markov Models (Kantas et al. 2015) or POMPs (King, Nguyen, and Ionides 2016)) can be viewed as a special case of these hidden-state models.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hidden-state models</span>"
    ]
  },
  {
    "objectID": "intro-hiddenstatemodels.html#definition-hidden-state-model",
    "href": "intro-hiddenstatemodels.html#definition-hidden-state-model",
    "title": "3  Hidden-state models",
    "section": "",
    "text": "3.1.1 Filtering and smoothing distributions\nBorrowing language from signal processing, we highlight two different posterior distributions for \\(R_t\\) that we may be interested in (Särkkä 2013).\nThe conditional2 filtering distribution is defined as:\n\\[\nP(X_t | y_{1:t}, \\theta)\n\\tag{3.3}\\]\nwhile the conditional smoothing distribution is defined as:\n\\[\nP(X_t | y_{1:T}, \\theta)\n\\tag{3.4}\\]\nThe filtering distribution uses only past observations to estimate the hidden-states, whereas the smoothing distribution uses both past and future observations.\n\n\n\nDiagram highlighting which data are used when finding the filtering and smoothing distributions of the hidden-states at time \\(t\\). The plotted curve is the observed data from which the filtering and smoothing distributions are derived.\n\n\nOne of the strengths of EpiFilter (Parag 2021) is its ability to find the smoothing distribution, allowing more data to inform \\(R_t\\) estimates, particularly improving inference in low-incidence scenarios.\nWe demonstrate SMC methods suitable for finding both distributions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hidden-state models</span>"
    ]
  },
  {
    "objectID": "intro-hiddenstatemodels.html#sec-intro-hiddenstatemodels-examples",
    "href": "intro-hiddenstatemodels.html#sec-intro-hiddenstatemodels-examples",
    "title": "3  Hidden-state models",
    "section": "3.2 Examples",
    "text": "3.2 Examples\nWe present three examples of increasing complexity. While all examples are relatively simple, many off-the-shelf methods are unable to fit the third example model.\n\n3.2.1 Example 1: a simple model\nIn Section 1.2 we introduced a simple model for \\(R_t\\) estimation. We can write this model in the form of a hidden-state model.\nThe Gamma\\((a_0, b_0)\\) prior distribution on \\(R_t\\) forms the state-space transition distribution:\n\\[\nR_t \\sim \\text{Gamma}(a_0, b_0)\n\\tag{3.5}\\]\nwhile the Poisson renewal model with mean \\(R_t \\Lambda_t\\) forms the observation distribution:\n\\[\nC_t | R_t, C_{1:t-1} \\sim \\text{Poisson}(R_t \\Lambda_t^c)\n\\tag{3.6}\\]\nThe sole time-varying hidden-state is the reproduction number \\(R_t\\), the observed data are reported cases \\(C_{1:T}\\), and the model parameters are \\(a_0\\), \\(b_0\\), and \\(\\{\\omega_u\\}_{u=1}^{u_{max}}\\). Altogether this forms our hidden-state model.\nWe previously solved this hidden-state model in Section 1.2 by leveraging the conjugacy of the state-space model with the observation model. Such conjugacy is rare, hence state-space models are often solved using SMC (or other simulation-based methods).\nSide-note: the state-space transition distribution for \\(R_t\\) does not depend on previous values of \\(R_t\\), so calling it a “transition” distribution might feel strange. Nevertheless, the model still fits in this framework!\n\n\n3.2.2 Example 2: EpiFilter\nEpiFilter (Parag 2021) is an example of a typical state-space model. Autocorrelation in \\(R_t\\) is modelled using a Gaussian random walk, defining the state-space model as:\n\\[\nR_t | R_{t-1} \\sim \\text{Normal}\\left(R_{t-1}, \\eta \\sqrt{R_{t-1}}\\right)\n\\tag{3.7}\\]\nwith initial condition \\(R_0 \\sim P(R_0)\\). The gaussian random walk acts to smooth \\(R_t\\) (see Section 1.2.2). Like Example 1 above, EpiFilter also uses the Poisson renewal model as the observation distribution:\n\\[\nC_t | R_t, C_{1:t-1} \\sim \\text{Poisson}(R_t \\Lambda_t^c)\n\\tag{3.8}\\]\nAs before, the hidden-states are the collection of \\(R_t\\) values, and the observed data are reported cases \\(C_{1:T}\\). Model parameters are now \\(\\eta\\) (which controls the smoothness of \\(R_t\\)), \\(\\{\\omega_u\\}_{u=1}^{u_{max}}\\), and \\(P(R_0)\\).\nNo analytical solution to the posterior distribution for \\(R_t\\) exists, however Parag (2021) avoid the need for simulation-based methods through the use of grid-based approximations.\n\n\n3.2.3 Example 3: Modelling infection incidence\nIn Section 1.4 we highlighted that the renewal model can be placed on either reported cases (as in the examples so far) or “true” infection incidence. Let’s now assume that reported cases are observed with Gaussian noise about the true number of infections.\nWe keep the same state-space model for \\(R_t\\), and now assume that infection incidence \\(I_t\\) (also a hidden-state) follows the renewal model, resulting in the following state-space model: \\[\nR_t | R_{t-1} \\sim \\text{Normal}\\left(R_{t-1}, \\eta \\sqrt{R_{t-1}}\\right)\n\\]\n\\[\nI_t | R_t, I_{1:t-1} \\sim \\text{Poisson}(R_t \\Lambda_t)\n\\]\nWith our observation distribution now taking the form:\n\\[\nC_t \\sim \\text{Normal}(I_t, \\phi)\n\\]\nWhile appearing simple, there are a number of reasons why existing methodology stuggles to fit this model. The new state-space model is non-Markovian, thus the grid-based approach used by EpiFilter (and many other methods developed for hidden Markov models) no longer works. Furthermore, \\(I_t\\) is integer-valued so the model cannot be fit using popular probabilistic programming languages like Stan.\n\n\n\n\nKantas, Nikolas, Arnaud Doucet, Sumeetpal S. Singh, Jan Maciejowski, and Nicolas Chopin. 2015. “On Particle Methods for Parameter Estimation in State-Space Models.” Statistical Science 30 (3): 328–51. https://doi.org/10.1214/14-STS511.\n\n\nKing, Aaron A., Dao Nguyen, and Edward L. Ionides. 2016. “Statistical Inference for Partially Observed Markov Processes via the R Package Pomp.” Journal of Statistical Software 69 (12). https://doi.org/10.18637/jss.v069.i12.\n\n\nParag, Kris V. 2021. “Improved Estimation of Time-Varying Reproduction Numbers at Low Case Incidence and Between Epidemic Waves.” PLOS Computational Biology 17 (9): e1009347. https://doi.org/10.1371/journal.pcbi.1009347.\n\n\nSärkkä, Simo. 2013. Bayesian Filtering and Smoothing. 1st ed. Cambridge University Press. https://doi.org/10.1017/CBO9781139344203.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hidden-state models</span>"
    ]
  },
  {
    "objectID": "intro-hiddenstatemodels.html#footnotes",
    "href": "intro-hiddenstatemodels.html#footnotes",
    "title": "3  Hidden-state models",
    "section": "",
    "text": "SMC methods can also be used to solve more standard parameter estimation problems, but this isn’t the focus of this website.↩︎\nThe specification of a conditional filtering/smoothing distribution is used to highlight that model parameters have been chosen instead of estimated. We later demonstrate how to find the marginal filtering/smoothing distribution which we denote with \\(P(X_t|y_{1:t})\\), to highlight that model parameters have been marginalised out.↩︎",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Hidden-state models</span>"
    ]
  },
  {
    "objectID": "smc-bootstrap.html",
    "href": "smc-bootstrap.html",
    "title": "4  The bootstrap filter",
    "section": "",
    "text": "4.1 Background\nThis chapter assumes fixed and known values of the model parameters. Chapter 5 considers the estimation of \\(\\theta\\).\nThe fundamental idea of the bootstrap filter (and particle filters in general) is to produce collections of samples from the target posterior distribution of a hidden-state model (Chapter 3) at each time-step \\(t\\). The result of this algorithm takes the form:\n\\[\n\\{x_t^{(i)}\\}_{i=1}^N \\sim P(X_t | y_{1:t}, \\theta)\n\\tag{4.1}\\]\nThese samples are often called particles, hence the term particle filtering.\nWe can use these particles to find the posterior mean, for example:\n\\[\nE[X_t|y_{1:t}, \\theta] \\approx \\frac{1}{N} \\sum_{i=1}^N x_t^{(i)}\n\\]\nor 95% credible intervals (by finding the 2.5th and 97.5th percentile values), or any other quantity we might care about. As these samples are Bayesian, they can be naturally used in downstream models.\nSide-note: the particles in Equation 4.1 represent the filtering distribution (our knowledge about \\(X_t\\) using data availabile up-to time \\(t\\), see Section 3.1.1). As we will soon see, we can also use the bootstrap filter to find particles representing the smoothing distribution (our knowledge about \\(X_t\\) using all available data).",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The bootstrap filter</span>"
    ]
  },
  {
    "objectID": "smc-bootstrap.html#background",
    "href": "smc-bootstrap.html#background",
    "title": "4  The bootstrap filter",
    "section": "",
    "text": "Goal\n\n\n\nThe goal of the bootstrap filter is to use observed data \\(y_{1:T}\\) to learn about the hidden-states \\(X_t\\) given a pre-determined parameter vector \\(\\theta\\).\n\n\n\n\n\n\n\n\n\n\n4.1.1 Overview\nThe bootstrap filter repeats three steps for each time step.\nAssume we are at time step \\(t\\) and have a collection of samples from the filtering distribution at time \\(t\\!-\\!1\\), denoted \\(\\{x_{t-1}^{(i)}\\}\\), representing our guesses of the value of \\(X_{t-1}\\). We update these samples to represent our guesses of the value of \\(X_t\\) by:\n\nProjecting them forward according to the state-space transition distribution.\nWeighting these projected particles according to the observation distribution.\nResampling these particles according to their weights.\n\nThis is visualised in Figure 4.1:\n\n\n\n\n\n\nFigure 4.1: Diagram showing the three steps performed by the bootstrap filter at each time step \\(t\\). In this diagram, particle histories are also resampled, making this an example of a bootstrap smoother.\n\n\n\nStep one creates a set of particles representing equally-likely one-step-ahead projections of \\(X_t\\) given data up to time \\(t-1\\):\n\\[\n\\{\\tilde{x}_t^{(i)}\\}_{i=1}^N \\sim P(X_t | y_{1:t-1}, \\theta)\n\\]\nStep two weights these projections according to how likely they are under the observation model:\n\\[\nw_t^{(i)} = P(y_t | \\tilde{x}_t^{(i)}, \\theta)\n\\]\nAnd the final step resamples according to these weights to ensure the resulting \\(\\{x_t^{(i)}\\}\\) are equally-likely samples from \\(P(X_t|y_{1:t}, \\theta)\\).\n\n\n4.1.2 Simulation-based inference\nIn order to use SMC methods to fit a hidden-state model, we only need to:\n\nSample from the state-space transition distribution\nEvaluate the observation distribution\n\nCrucially, we do not need to evaluate the state-space transition distribution, allowing for easy simulation-based inference.\nSimulation-based inference methods rely on simulations rather than analytical solutions and are very popular in statistical epidemiology. In this case, sampling from the state-space transition distribtion is the same as running the “simulator” one-step forward into the future. This simulator implies a likelihood even though we never need to find its precise form.",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The bootstrap filter</span>"
    ]
  },
  {
    "objectID": "smc-bootstrap.html#the-bootstrap-filter",
    "href": "smc-bootstrap.html#the-bootstrap-filter",
    "title": "4  The bootstrap filter",
    "section": "4.2 The bootstrap filter",
    "text": "4.2 The bootstrap filter\nLike before, assume that we have a collection of samples from the filtering distribution at time \\(t\\!-\\!1\\), denoted \\(\\{x_{t-1}^{(i)}\\} \\sim P(X_{t-1}|y_{1:t-1}, \\theta)\\). For now, also assume that our state-space transitions are Markovian (i.e., \\(X_t\\) depends only on \\(X_{t-1}\\)).\nStep 1: Projection\nBy coupling these particles with the state-space transition model (Equation 3.1), we can make one-step-ahead predictions. Mathematically we want to obtain the predictive distribution: \\[\n\\underbrace{P(X_t | y_{1:t-1}, \\theta)}_{\\text{predictive dist.}} = \\int \\underbrace{P(X_t | X_{t-1}, \\theta)}_{\\text{state-space transition dist.}} \\underbrace{P(X_{t-1}|y_{1:t-1}, \\theta)}_{\\text{prev. filtering dist.}} \\ dX_{t-1}\n\\]\nWhich is represented by complicated and potentially high-dimensional integral of two things we know. Fortunately, all we need to do is sample from the state-space transition model while conditioning on our particles:\n\\[\n\\tilde{x}_t^{(i)} \\sim P(X_t | x_{t-1}^{(i)}, \\theta)\n\\]\nStep 2: Weighting\nIf we treat this predictive distribution as the prior distribution for \\(X_t|y_{1:t}\\), we can apply Bayes’ formula:\n\\[\n\\underbrace{P(X_t | y_{1:t}, \\theta)}_{\\text{new filtering dist}} \\propto \\underbrace{P(y_t|X_t, y_{1:t-1}, \\theta)}_{\\text{observation dist.}} \\underbrace{P(X_t | y_{1:t-1}, \\theta)}_{\\text{predictive dist.}}\n\\]\nSince we already have samples from the predictive distribution \\(P(X_t | y_{1:t-1}, \\theta)\\), all we need to do is assign them weights \\(w_t^{(i)}\\) according to the observation distribution:\n\\[\nw_t^{(i)} = P(y_t | \\tilde{x}_t^{(i)}, y_{1:t-1}, \\theta)\n\\]\nStep 3: Resampling\nThe final set of particles \\(\\{x_t^{(i)}\\}\\) is constructed by sampling (with replacement) from \\(\\{\\tilde{x}_t^{(i)}\\}\\) according to the corresponding weights, resulting in:\n\\[\nx_t^{(i)} \\sim P(X_t | y_{1:t}, \\theta)\n\\]\nRepeat\nStarting with an initial set of particles \\(\\{x_0^{(i)}\\} \\sim P(X_0)\\), all we need to do is repeat this procedue for each \\(t = 1, 2, \\ldots, T\\), storing the particle values as we go. This concludes the bootstrap filter.\nThe bootstrap filter is a special case of a sequential importance sampling algorithm (Section 4.6).",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The bootstrap filter</span>"
    ]
  },
  {
    "objectID": "smc-bootstrap.html#fixed-lag-smoothing",
    "href": "smc-bootstrap.html#fixed-lag-smoothing",
    "title": "4  The bootstrap filter",
    "section": "4.3 Fixed-lag smoothing",
    "text": "4.3 Fixed-lag smoothing\nThere are two problems with the bootstrap filter above:\n\nIt assumes the state-space transition model is Markovian (\\(X_t\\) depends only on \\(X_{t-1}\\))\n\\(X_t\\) is informed by past data \\(y_{1:t}\\) only (the filtering posterior), when we may want to leverage all available data (the smoothing posterior).\n\nBoth of these problems are solved using fixed-lag resampling.",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The bootstrap filter</span>"
    ]
  },
  {
    "objectID": "smc-bootstrap.html#algorithm",
    "href": "smc-bootstrap.html#algorithm",
    "title": "4  The bootstrap filter",
    "section": "4.4 Algorithm",
    "text": "4.4 Algorithm\nWe present the algorithm as a Julia code-snippet. For a pseudocode implementation, please see our preprint.\nThe entire algorithm can be written in seven lines of code, although we use eight and some comments below for clarity. To fit a specific model, simply replace the initial distribution pX0, the state-space transition distribution pXt, and observation distribution pYt, with those of the hidden-state model.\n# Inputs:\n#  - θ: a parameter vector\n#  - y: a vector of data\n#  - N: the number of particles\n#  - L: fixed-lag resampling window length\n\nfunction SimpleBootstrapFilter(θ, y, N, L)\n    \n     # Pre-allocate an empty matrix of particles\n    X = zeros(N, length(Ct))\n\n    # Sample initial values\n    X[:,1] = rand(pX0, N) \n    \n    # For each time step:\n    for tt = 2:length(Ct)\n\n        # Project the hidden states\n        X[:,tt] = rand.(pXt(X[:,tt-1], θ))\n\n        # Calculate weights\n        W = pdf.(pYt(X[:,tt], θ), y[tt])\n\n        # Resample\n        indices = wsample(1:N, W, N)\n        X[:, max(tt-L, 1):tt] = X[indices, max(tt-L, 1):tt]\n\n    end\n    \n    return(R, Y)\n    \nend",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The bootstrap filter</span>"
    ]
  },
  {
    "objectID": "smc-bootstrap.html#example",
    "href": "smc-bootstrap.html#example",
    "title": "4  The bootstrap filter",
    "section": "4.5 Example",
    "text": "4.5 Example\nFor demonstration, we consider a simple reproduction number estimator. First assume that \\(\\log R_t\\) follows a Gaussian random walk with standard deviation \\(0.2\\):\n\\[\n\\log R_t | R_{t-1} \\sim \\text{Normal}(\\log R_{t-1}, 0.2)\n\\]\nand then assume that reported cases follow the Poisson renewal model (Equation 1.2):\n\\[\nC_t | R_t, C_{1:t-1} \\sim \\text{Poisson}\\left(R_t \\sum_{u=1}^{u_{max}} C_{t-u} \\omega_u\\right)\n\\]\nFinally, we use a \\(\\text{Uniform}(0, 10)\\) initial distribution for \\(R_t\\). This behaves somewhat like a prior distribution, except that resmapling ensures the early estimates of \\(R_t\\) reflect later data, so this has limited impact on the final estimates.\nWe load the same data used in the first two example models of the preprint. The entire algorithm (implemented below) takes approximately 0.4s to run on an M1 MacBook Pro.\n\nusing StatsBase, Distributions, Plots, Measures\n\ninclude(\"../src/LoadData.jl\")\n\nfunction runSimpleExample()\n    \n    # Setup:\n    N = 10000 # Number of particles\n    T = 100 # Number of time steps\n    ω = pdf.(Gamma(2.36, 2.74), 1:T) # Serial interval\n    Y = loadData(\"NZCOVID\") # Load data (Y is a DataFrame with column Ct)\n    L = 40 # Fixed-lag resampling window\n\n    # Pre-allocate and sample initial values:\n    R = zeros(N, T)\n    R[:,1] = log.(rand(Uniform(0, 10), N))\n\n    # Run the bootstrap filter:\n    for tt = 2:T\n        R[:,tt] = rand.(Normal.(R[:,tt-1], 0.2)) # Project Rt\n        Λ = sum(Y.Ct[(tt-1):-1:1] .* ω[1:(tt-1)]) # Calculate the force-of-infection\n        W = pdf.(Poisson.(Λ * exp.(R[:,tt])), Y.Ct[tt]) # Calculate weights\n        R[1:N, max(tt-L, 1):tt] = R[wsample(1:N, W, N), max(tt-L, 1):tt] # Resample\n    end\n    \n    # Process and plot the results\n    m = vec(mean(exp.(R), dims=1))\n    l = [quantile(Rt, 0.025) for Rt in eachcol(exp.(R))]\n    u = [quantile(Rt, 0.975) for Rt in eachcol(exp.(R))]\n    plot(Y.date, m, ribbon=(m-l, u-m), size=(800,400), xlabel=\"Date\", ylabel=\"Reproduction number\", label=false, margins=3mm)\n\nend\n\nrunSimpleExample()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia\n\n\n\nThe total elapsed time is \\(&lt;1\\)s. If running this example locally, the first time may take longer as Julia must compile the function and load the plotting backend. This long “time-to-first-plot” is a well-known annoyance in Julia.\n\n\n\n4.5.1 Choosing the number of particles\nIn the example above, we used \\(N = 10{,}000\\) particles. Heuristically, we can check whether \\(N\\) is large enough by refitting the model and checking that our results are visually stable.\nAlternatively, the effective sample size (ESS) is a measure of the number of independent samples that would be equivalent to a given weighted sample (Elvira, Martino, and Robert 2022). It is typically written as:\n\\[\n\\hat{ESS} = \\frac{1}{\\sum_{i=1}^N \\bar{w}_i^2}\n\\]\nwhere \\(\\bar{w}_i\\) is the normalised weight of the \\(i^{th}\\) sample. As we resample past states without tracking particle weights we cannot directly apply this formula. Instead, we approximate it using:\n\\[\n\\hat{ESS} \\approx \\frac{1}{\\sum_{j=1}^M \\left(n_j/N\\right)^2}\n\\]\nwhere \\(n_j\\) is the number of times that the \\(j^{th}\\) unique value features in the resampled set of particles (and \\(M\\) is the total number of unique particles after resampling). We calculate this after running the bootstrap filter using:\n\nfunction approximateESS(Rvec)\n\n    N = length(Rvec)\n\n    # Fetch a dictionary of the unique values of Rvec and the count\n    nj = countmap(vec(Rvec))    \n\n    # Compute ESS approximation\n    ESS = 1 / sum([(v/N)^2 for v in values(nj)])\n\n    return(ESS)\n\nend\n\nWe apply this to each column of R in the example above to estimate the ESS (full code is collapsed for brevity):\nESS = [approximateESS(Rt) for Rt in eachcol(R)]\n\n\nCode\nusing StatsBase, Distributions, Plots, Measures\n\ninclude(\"../src/LoadData.jl\")\n\nfunction runSimpleExampleWithESS()\n    \n    # Setup:\n    N = 10000 # Number of particles\n    T = 100 # Number of time steps\n    ω = pdf.(Gamma(2.36, 2.74), 1:T) # Serial interval\n    Y = loadData(\"NZCOVID\") # Load data (Y is a DataFrame with column Ct)\n    L = 40 # Fixed-lag resampling window\n\n    # Pre-allocate and sample initial values:\n    R = zeros(N, T)\n    R[:,1] = log.(rand(Uniform(0, 10), N))\n\n    # Run the bootstrap filter:\n    for tt = 2:T\n        R[:,tt] = rand.(Normal.(R[:,tt-1], 0.2)) # Project Rt\n        Λ = sum(Y.Ct[(tt-1):-1:1] .* ω[1:(tt-1)]) # Calculate the force-of-infection\n        W = pdf.(Poisson.(Λ * exp.(R[:,tt])), Y.Ct[tt]) # Calculate weights\n        R[1:N, max(tt-L, 1):tt] = R[wsample(1:N, W, N), max(tt-L, 1):tt] # Resample\n    end\n    \n    # Process and plot the effective sample size\n    ESS = [approximateESS(Rt) for Rt in eachcol(R)]\n    plot(Y.date, ESS, size=(800,400), xlabel=\"Date\", ylabel=\"Effective sample size\", label=false, margins=3mm)\n\nend\n\nrunSimpleExampleWithESS()\n\n\n\n\n\n\n\n\n\nThe ESS decreases rapidly as we move left along the x-axis. In general we expect the ESS at all \\(t &lt; T - L\\) to be similar (i.e. the plot of ESS against time should be flat until the final \\(L\\) time steps). In this example, the data are more unpredictable in earlier time-steps, which causes the projection step to select fewer “good” particles, thus also decreasing the ESS.\nIn this example, the minimum ESS is 36, which results in unstable credible intervals for estimates at small \\(t\\). The ESS can be increased by:\n\nIncreasing \\(N\\)\nDecreasing \\(L\\)\nFitting a “better” model (one where the projections are more likely)\nUsing more complex tools from the SMC literature, such as an auxillary particle filter, which uses future data in the proposal density (Doucet and Johansen 2011)\n\nIn this case, increasing \\(N\\) to \\(100{,}000\\) only adds a couple of seconds of computation time, and results in much more stable estimates (a minimum ESS of approximately 360).",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The bootstrap filter</span>"
    ]
  },
  {
    "objectID": "smc-bootstrap.html#sec-smc-bootstrap-additional",
    "href": "smc-bootstrap.html#sec-smc-bootstrap-additional",
    "title": "4  The bootstrap filter",
    "section": "4.6 Additional resources",
    "text": "4.6 Additional resources\nBootstrap filters (and SMC methods more generally) have found use in many fields. Each field has their own motivation for and notation describing these methods. We provide an overview of other resources here.\nBayesian Filtering and Smoothing (Särkkä 2013)\nThose with an engineering background may be familiar with “filtering and smoothing”, where the state of a time-varying system is tracked through the observation of noisy measurements. Classical examples include GPS position tracking or audio signal processing.\nThe Kalman filter, which provides an analytical solution when the state-space transition and observation models are linear Gaussian and Markovian, is perhaps the best-known example of a filtering method from engineering.\nChapters 7 and 11 of Särkkä (2013) introduce SMC methods under the headings “particle filtering” and “particle smoothing”. We also recommend chapters 1 (What are Bayesian filtering and smoothing?), 4 (Bayesian filtering equations and exact solutions), and 8 (Bayesian smoothing equations and exact solutions).\nA survey of Sequential Monte Carlo Methods for Economics and Finance (Creal 2012)\nThose with an econometrics background may find this extensive review helpful, although the author focusses on Markovian models. Examples employed in this review include a stochastic volatility model and a nonlinear dynamic stochastic general equilibrium model.\nData Assimilation Fundamentals: … (Evensen, Vossepoel, and Van Leeuwen 2022)\nThose with a background in atmospheric science, oceanography, meteorology, or other environmental sciences may be familiar with “data assimilation”, where focus is placed on combining model predictions with observational data. Chapter 9 of this book introduces particle filters as a method for solving the “fully nonlinear data assimilation” problem.\nThis list is incomplete. If you know of any additional resources that may be helpful, please get in touch!\n\n\n\n\nCreal, Drew. 2012. “A Survey of Sequential Monte Carlo Methods for Economics and Finance.” Econometric Reviews 31 (3): 245–96. https://doi.org/10.1080/07474938.2011.607333.\n\n\nDoucet, Arnaud, and Adam M. Johansen. 2011. “A Tutorial on Particle Filtering and Smoothing : Fiteen Years Later.” In, edited by Dan Crisan and Boris Rozovskii, 656–705. Oxford ; N.Y.: Oxford University Press.\n\n\nElvira, Víctor, Luca Martino, and Christian P. Robert. 2022. “Rethinking the Effective Sample Size.” International Statistical Review 90 (3): 525–50. https://doi.org/10.1111/insr.12500.\n\n\nEvensen, Geir, Femke C. Vossepoel, and Peter Jan Van Leeuwen. 2022. Data Assimilation Fundamentals: A Unified Formulation of the State and Parameter Estimation Problem. Springer Textbooks in Earth Sciences, Geography and Environment. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-96709-3.\n\n\nSärkkä, Simo. 2013. Bayesian Filtering and Smoothing. 1st ed. Cambridge University Press. https://doi.org/10.1017/CBO9781139344203.",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>The bootstrap filter</span>"
    ]
  },
  {
    "objectID": "smc-pmmh.html",
    "href": "smc-pmmh.html",
    "title": "5  Parameter estimation",
    "section": "",
    "text": "5.1 Likelihood estimation\nIn Chapter 4 we introduced the bootstrap filter, an algorithm which returns a particle approximation to \\(P(X_t | y_{1:T}, \\theta)\\). Critically, this posterior distribution depends on parameter(s) \\(\\theta\\). This chapter is dedicated to performing inference on \\(\\theta\\) itself.\nWe start by showing how the bootstrap filter can be used to estimate the log-likelihood of \\(\\theta\\). We then use this log-likelihood (and a corresponding prior distribution) to develop a posterior distribution for \\(\\theta\\). Two approaches for this are demonstrated:\nIf your goal is to perform inference on \\(\\theta\\), then you are done! In the frequentist setting, you have access to a likelihood upon which you can base your inferences (e.g. Section 5.5.2). In the Bayesian setting you have the desired posterior distribution.\nIf your goal is to perform inference on \\(X_t\\) after accounting for uncertainty in \\(\\theta\\), then you can use the posterior distribution to marginalise out \\(\\theta\\). This is demonstrated in Chapter 6.\nThe log-likelihood of \\(\\theta\\) is defined as:\n\\[\n\\ell(\\theta|y_{1:T}) = \\log P(y_{1:T}|\\theta) = \\sum_{t=1}^T \\log P(y_t| y_{1:t-1}, \\theta)\n\\tag{5.1}\\]\nThe second equality in Equation 5.1 is called the predictive decomposition of the likelihood, which decomposes the likelihood into one-step-ahead predictions.\nWe draw attention to the fact that our hidden-states \\(X_t\\) do not feature in Equation 5.1, whereas our model (the state-space and observation distributions) depend heavily on these variables. This is where the predictive decomposition is useful, as the bootstrap filter produces a convenient way of estimating \\(P(y_t| y_{1:t-1}, \\theta)\\), at no extra cost.\nFirst we write the predictive distribution as an expectation of the observation distribution \\(P(y_t|X_{1:t}, y_{1:t-1}, \\theta)\\): \\[\n\\begin{align}\nP(y_t | y_{1:t-1}, \\theta) &= \\int P(y_t | X_{1:t}, y_{1:t-1}, \\theta) P(X_{1:t} | y_{1:t-1}, \\theta) \\ dX_{1:t}\\\\\n&= E_{X_{1:t}|y_{1:t-1}, \\theta}\\left[P(y_t|X_{1:t}, y_{1:t-1}, \\theta)\\right]\n\\end{align}\n\\tag{5.2}\\]\nThis expectation is taken with respect to \\(X_{1:t}\\), conditional on \\(y_{1:t-1}\\) and \\(\\theta\\). The bootstrap filter generates samples of these \\(X_{1:t}\\) in the projection step (?eq-smc-bootstrapprojection), denoted \\(\\tilde{x}_{1:t}^{(i)}\\). Thus, we can approximate the expectation (and thus the predictive likelihood) using: \\[\nP(y_t | y_{1:t-1}, \\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N P\\left(y_t | \\tilde{x}_{1:t}^{(i)}, y_{1:t-1}, \\theta\\right)\n\\tag{5.3}\\]\nFinally, we highlight that \\(P\\left(y_t | \\tilde{x}_{1:t}^{(i)}, y_{1:t-1}, \\theta\\right)\\) is precisely the weight of the \\(i^{th}\\) particle at time-step \\(t\\), and thus:\n\\[\nP(y_t |y_{1:t-1}, \\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N w_t^{(i)} = \\bar{w}_t\n\\tag{5.4}\\]\nCombining equations Equation 5.1 and Equation 5.4 we find:\n\\[\n\\hat{\\ell}(\\theta|y_{1:T}) = \\sum_{t=1}^T \\log \\bar{w}_t\n\\tag{5.5}\\]\nThese weights are calculated within the bootstrap filter, so approximating the log-likelihood simply requires us to calculate and store the average weights at each step. Compared to the core bootstrap filter, this computation is trivial.",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parameter estimation</span>"
    ]
  },
  {
    "objectID": "smc-pmmh.html#sec-smc-likelihoodest",
    "href": "smc-pmmh.html#sec-smc-likelihoodest",
    "title": "5  Parameter estimation",
    "section": "",
    "text": "5.1.1 Example\nWe continue with the example from ?sec-smc-bootstrapexample. For simplicity, we package up the bootstrap into a single function that returns a matrix of hidden-state samples \\(X\\) and a same-sized matrix of weights \\(W\\) (click on the arrow to expand code).\n\n\nCode\nusing Distributions, Plots, Measures\ninclude(\"../src/loadData.jl\")\nnzdata = loadData(\"NZCOVID\")\n\n# Specify the serial interval and initial distribution for Rt\nω = pdf.(Gamma(2.36, 2.74), 1:100)\nω = ω/sum(ω)\npR0 = Uniform(0, 10)\n\n# Create the function to run the boostrap-filter\nfunction runBootstrapFilter(σ, nzdata; N=10000, L=50)\n\n    # Initialise output matrices\n    T = length(nzdata.Ct)\n    X = zeros(N, T)\n    W = zeros(N, T)\n\n    # Sample from initial distribution\n    X[:,1] = rand.(pR0, N)\n\n    # Run the filter\n    for tt = 2:T\n\n        # Project according to the state-space model\n        X[:,tt] = exp.(rand.(Normal.(log.(X[:,tt-1]), σ)))\n\n        # Weight according to the observation model\n        Λ = sum(nzdata.Ct[tt-1:-1:1] .* ω[1:tt-1])\n        W[:,tt] = pdf.(Poisson.(X[:,tt] .* Λ), nzdata.Ct[tt])\n\n        # Resample\n        inds = wsample(1:N, W[:,tt], N; replace=true)\n        X[:, max(tt - L, 1):tt] = X[inds, max(tt - L, 1):tt]\n\n    end\n\n    return(X, W)\n\nend\n\n\nrunBootstrapFilter (generic function with 1 method)\n\n\nThe log-likelihood of \\(\\sigma\\) is estimated by summing the logarithm of the columnwise means of \\(W\\), ignoring the first column as we do not calculate weights for this time-step (the renewal model requires at least one day of past data to calculate \\(\\Lambda_t^c\\)):\n\nσ = 0.1\n(X, W) = runBootstrapFilter(σ, nzdata)\nloglik = sum(log.(mean(W, dims=1)[2:end]))\nprintln(\"Estimated log-likelihood = $loglik\")\n\nEstimated log-likelihood = -220.70446122703086\n\n\n\n\n5.1.2 Variance of log-likelihood estimates\n[#TODO: add a note about why we can use much smaller \\(N\\). The ESS in this case is much higher as we have not yet resmapled. etc etc]\nWe use \\(\\hat{\\ell}\\) to emphasise that Equation 5.5 is an estimate of the likelihood \\(\\ell\\). Practically speaking, the variance of this estimator depends on:\n\nHow well the model fits: if the projection-step places particles in more plausible regions of the support of the observation-distribution the variance of the log-weights decreases.\nThe dimensionality of the observations: higher dimenionsal observation-distributions spread probability mass over a wider area, increasing the variance of log-weights.\nThe length of data: the likelihood estimator is a sum of stochastic terms, so the variance scales approximately linearly with \\(T\\).\nThe number of particles used: more particles increases the effective sample size at each time-step, decreasing the variance of the log-likelihood estimator.\n\nWe will demonstrate this variability in section Section 5.2, although highlight that the PMMH algorithm outlined in Section 5.3 is somewhat robust to this variability.\nThe bootstrap filter outlined above used \\(N = 10000\\) particles for \\(T = 100\\) days. The standard deviation of the log-likelihood estimator can itself be (crudely) estimated by:\n\nlogliks = zeros(100)\nThreads.@threads for ii = 1:100 # The Threads macro is used to leverage multiple cores\n    (X, W) = runBootstrapFilter(σ, nzdata)\n    logliks[ii] = sum(log.(mean(W, dims=1)[2:end]))\nend\nprintln(\"Sample variance of log-lik estimates = $(std(logliks))\")\n\nSample variance of log-lik estimates = 0.44892089421787496",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parameter estimation</span>"
    ]
  },
  {
    "objectID": "smc-pmmh.html#sec-smc-gridbased",
    "href": "smc-pmmh.html#sec-smc-gridbased",
    "title": "5  Parameter estimation",
    "section": "5.2 Grid-based posterior distribution",
    "text": "5.2 Grid-based posterior distribution\nIn the simplest case, where \\(\\theta\\) is 1-D and discrete (with a sufficiently small range), we can simply estimate \\(\\ell(\\theta|y_{1:T})\\) on all values of \\(\\theta\\). If \\(\\theta\\) is 1-D and continuous, then we can estimate it on a grid of values. Given prior distribution \\(P(\\theta)\\), Bayes’ formula can be used to estimate \\(P(\\theta|y_{1:T})\\).\nIn the grid-based case, if we assume a discrete prior distribution over the grid, then our posterior distribution is exact. If we approximate our prior distribution with a discretised prior distribution, then our posterior distribution is also an approximation - although the difference is purely philosophical.\n\n5.2.1 Example (continued)\nWe can run the bootstrap filter on a range of values of \\(\\sigma\\). We also store the values of all particles so we can later visualise \\(P(R_t|y_{1:T}, \\sigma)\\) for each value of \\(\\sigma\\):\n\nN = 100000 # We use additional particles so the algorithm works well even when using poor choices of σ\nσvalues = 0.06:0.01:0.4\nlogliks = zeros(length(σvalues))\nX = zeros(N, length(nzdata.Ct), length(σvalues))\nfor (ii, σ) in enumerate(σvalues)\n    (Xi, W) = runBootstrapFilter(σ, nzdata; N=N)\n    logliks[ii] = sum(log.(mean(W, dims=1)[2:end]))\n    X[:,:,ii] = Xi\nend\n\nIf we assume a discrete uniform prior distribution for σvalues, we can calculate the posterior distribution as follows:\n\nσposterior  = exp.(logliks .- maximum(logliks))\nσposterior = σposterior/sum(σposterior)\n\nPlotting the log-likelihood of \\(\\sigma\\) alongside the filtering distribution for \\(R_t\\) highlights that \\(\\sigma = 0.1\\) is probably too low, and results in substantially different estimates of \\(R_t\\) relative to estimates using more likely values of \\(\\sigma\\).\n\n\nCode\n# # Make animated plot of filtering values\n# σvalues_str = string.(σvalues)\n# (σvalues_str[3], σvalues_str[8], σvalues_str[13], σvalues_str[18]) = (\"0.10\", \"0.20\", \"0.30\", \"0.40\")\n\nanim = @animate for (ii, σ) in enumerate(σvalues)\n\n    animPlot = plot(layout=grid(2,1, heights=[0.5, 0.5]), size=(600, 400), left_margin=3mm, bottom_margin=3mm)\n    \n    # Plot log-lik\n    barcolors = repeat([:darkorange], length(σvalues))\n    barcolors[ii] = :darkgreen\n    bar!(animPlot[1], σvalues, σposterior, label=false, fill=barcolors, linewidth=3)\n    # vline!(animPlot[1], [σ], label=false)\n    # ylims!(animPlot[1], (-220, -209))\n    xlabel!(animPlot[1], \"σ\")\n    ylabel!(animPlot[1],\"P(σ|y_{1:T})\")\n    \n    # Plot Rt estimates\n    T = size(X)[2]\n    m = [mean(X[:,tt,ii]) for tt in 1:T]\n    l = [quantile(X[:,tt,ii], 0.025) for tt in 1:T]\n    u = [quantile(X[:,tt,ii], 0.975) for tt in 1:T]\n    plot!(animPlot[2], nzdata.date, m, ribbon=(m-l, u-m), label=false, color=:darkgreen)\n    hline!(animPlot[2], [1], label=false, color=:black)\n    ylims!(animPlot[2], (0, 9))\n    xlabel!(animPlot[2], \"Date\")\n    # ylabel!(animPlot[2], \"Filtering Rt | σ = $(σvalues_str[ii])\")\n    \nend\n\ngif(anim, fps=5, verbose=false, show_msg=false)\n\n\n\n\n\n\n\nFigure 5.1: Estimates of the log-likelihood of sigma (upper) and the corresponding estimates of Rt for the first 100 days of the COVID-19 pandemic.",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parameter estimation</span>"
    ]
  },
  {
    "objectID": "smc-pmmh.html#sec-smc-pmmh",
    "href": "smc-pmmh.html#sec-smc-pmmh",
    "title": "5  Parameter estimation",
    "section": "5.3 Particle marginal Metropolis Hastings",
    "text": "5.3 Particle marginal Metropolis Hastings\n\n5.3.1 Algorithm\n\n\n5.3.2 Example (continued)",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parameter estimation</span>"
    ]
  },
  {
    "objectID": "smc-pmmh.html#concluding-remarks",
    "href": "smc-pmmh.html#concluding-remarks",
    "title": "5  Parameter estimation",
    "section": "5.4 Concluding remarks",
    "text": "5.4 Concluding remarks\nThe estimates of \\(\\theta\\) as presented here assume that the model is correctly specified, however this almost certainly is not the case.\nIn the context of the example, we highlight the presence of unmodelled observation noise as a particular concern here: any noise in the data beyond that implied by the Poisson observation distribution is attributed to changes in \\(R_t\\) (that is, is incorporated in estimates of \\(\\sigma\\)). There are almost certainly additional sources of noise, so we are likely over-estimating \\(\\sigma\\). This is dicscussed further in Chapter 9.",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parameter estimation</span>"
    ]
  },
  {
    "objectID": "smc-pmmh.html#other-methods",
    "href": "smc-pmmh.html#other-methods",
    "title": "5  Parameter estimation",
    "section": "5.5 Other methods",
    "text": "5.5 Other methods\nPMMH is just one method for estimating fixed model parameters \\(\\theta\\). Alternatives include SMC^2 (an “online” Bayesian approach) and iterated filtering (a frequentist approach). We provide a brief description of these methods here. Further discussion of parameter estimation in state-space models can be found in Kantas et al. (2015).\n\n5.5.1 SMC^2\n#TODO: Write SMC^2 section\n\n\n5.5.2 Iterated filtering and POMP\nIterated filtering (E. L. Ionides, Bretó, and King 2006; Edward L. Ionides et al. 2011; Edward L. Ionides et al. 2015) takes a frequentist approach to estimating fixed model parameters \\(\\theta\\). This is a direct alternative to the PMMH algorithm we introduce above.\nTheoretically, this algorithm can estimate parameter vectors that would otherwise be intractable in a Bayesian setting. Their examples are restricted to Markovian models, limiting their applicability to renewal models, although including the relevant state-history in \\(X_t\\) is a potential way around this problem (at the cost of a substantially increased model dimensionality).\nPOMP is the name given to an R package developed by Aaron King, Dao Nguyen, and Edward Ionides (King, Nguyen, and Ionides 2016) for performing statistical inference on “Partially observed markov processes (POMP)”. The package contains a collection of methods, including PMMH[#TODO: CHECK], although the focus here is on iterated filtering (called IF2 in the package).\nThe authors use different terminology in places. POMP refers to Hidden Markov Models (HMMs), which are hidden-state models with Markovian hidden-state transitions. “Plug-and-play” refers to simulation-based inference methods (i.e. those that only require a simulator of the model, rather than direct evaluations of a likelihood function).\n\n5.5.2.1 Main differences\nIterated filtering methods [] [] are claimed to be the only currently available, full-information (see below), simulation-based, frequentist methods for HMMs.\n\n\n5.5.2.2 The “three criteria”\nThe authors emphasise three criteria for comparing inference methods on HMMs:\n\nThe “plug-and-play” property\nFull-information or feature-based\nFrequentist or Bayesian\n\nThe “plug-and-play” property refers to simulation-based inference: a method is called “plug-and-play” if it only requires a simulator of the hidden-state model, rather than explicit likelihood evaluations.\nFull-information methods are those based on the likelihood function for the full data, whereas feature-based methods consider summary statistics or an alternative to the likelihood. Generally, full-information models are preferred, although feature-based methods can be useful if they improve computational efficiency.\nThe primary difference between their methodology and ours, is their focus on frequentist-inference.\nThe methods introduced on this website are simulation-based (so satisfy the “plug-and-play” criteria), generally leverage full-information, and are Bayesian. The methods presented in POMP are similar in the first two regards, although are generally (but not always) frequentist in nature.\nThere are obvious cases where we do not leverage full-information, e.g. when working with temporally aggregated data. However, in these cases the approximation occurs in the model, rather than the method, meaning our methods still leverage full-information in this context.\n\n\n\n\nIonides, E. L., C. Bretó, and A. A. King. 2006. “Inference for Nonlinear Dynamical Systems.” Proceedings of the National Academy of Sciences 103 (49): 18438–43. https://doi.org/10.1073/pnas.0603181103.\n\n\nIonides, Edward L., Anindya Bhadra, Yves Atchadé, and Aaron King. 2011. “Iterated Filtering.” The Annals of Statistics 39 (3). https://doi.org/10.1214/11-AOS886.\n\n\nIonides, Edward L., Dao Nguyen, Yves Atchadé, Stilian Stoev, and Aaron A. King. 2015. “Inference for Dynamic and Latent Variable Models via Iterated, Perturbed Bayes Maps.” Proceedings of the National Academy of Sciences 112 (3): 719–24. https://doi.org/10.1073/pnas.1410597112.\n\n\nKantas, Nikolas, Arnaud Doucet, Sumeetpal S. Singh, Jan Maciejowski, and Nicolas Chopin. 2015. “On Particle Methods for Parameter Estimation in State-Space Models.” Statistical Science 30 (3): 328–51. https://doi.org/10.1214/14-STS511.\n\n\nKing, Aaron A., Dao Nguyen, and Edward L. Ionides. 2016. “Statistical Inference for Partially Observed Markov Processes via the R Package Pomp.” Journal of Statistical Software 69 (12). https://doi.org/10.18637/jss.v069.i12.",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Parameter estimation</span>"
    ]
  },
  {
    "objectID": "smc-combining.html",
    "href": "smc-combining.html",
    "title": "6  Putting it together",
    "section": "",
    "text": "6.1 Supplementary results\n#TODO: Finish page.",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Putting it together</span>"
    ]
  },
  {
    "objectID": "smc-combining.html#supplementary-results",
    "href": "smc-combining.html#supplementary-results",
    "title": "6  Putting it together",
    "section": "",
    "text": "6.1.1 Varying fixed-lag resampling \\(L\\)\nFixed-lag resampling introduces a potential source of bias. Fundamentally, we are approximating the smoothing posterior \\(P(X_t|y_{1:T})\\) with \\(P(X_t|y_{1:t+L})\\), ignoring any direct association between the current hidden state \\(X_t\\) and data collected more than \\(L\\) steps in the future. This can be an issue if the process does not “forget” fast enough (for example, if \\(R_t\\) varies very slowly over time), or if \\(L\\) is smaller than the maximum1 generation time.\nCompared to hidden-state estimation, the choice of \\(L\\) is generally less important when estimating fixed parameters using PMMH, as only the filtering distribution \\(P(X_t|y_{1:t})\\) is required for this task. In the Markovian setting, all past-state resampling could be disabled entirely, but as the renewal model is inherently non-Markovian, past-state resampling is required. However, particle degeneracy is not a concern, so \\(L\\) could be set to \\(t\\) (full past-state resampling), but this is computationally expensive. Using fixed-lag resampling thus strikes a balance between these two extremes, provided \\(L\\) is large enough that hidden-state trajectories are consistent over the maximum generation time.\nTo test the effect of varying \\(L\\), we fit the third example model from Section 8.3, with \\(L = 5, 10, 20, 40, 80\\). For context, the mean generation time is assumed to be 6.5 days, with \\(L = 20\\) covering 98.8% of the probability mass of the generation time distribution. The similarity of posterior means and 95% credible intervals of the parameters \\(\\sigma\\) and \\(\\phi\\) suggest there is little systematic impact of \\(L\\) (the variation observed arises from Monte Carlo error) on parameter estimation in this example. Greater variation is seen in estimates of the reproduction number at low \\(t\\), where there is limited data. Despite this, there is a high level of agreement between estimates for later estimates, despite the known problems with using \\(L &lt; 20\\).\n\n\nCode\nusing Plots; gr()\nusing Measures, CSV, DataFrames\n\n\ninclude(\"../src/LoadData.jl\")\ninclude(\"../src/PMMH.jl\")\ninclude(\"../src/FitModel.jl\")\ninclude(\"../src/MarginalPosterior.jl\")\ninclude(\"../src/Support.jl\")\n\nY = loadData(\"NZCOVID\")\n\nfunction ExampleModel3(θ, Y::DataFrame, opts::Dict)\n    \n    # Extract frequently used options\n    T = opts[\"T\"] # Number of time steps\n    N = opts[\"N\"] # Number of particles to use\n    L = opts[\"L\"] # Length of fixed-lag resampling\n    \n    # Define the serial interval\n    ω = pdf.(Gamma(2.36, 2.74), 1:100) # (Unnormalised) serial interval\n    ω = ω/sum(ω) # Normalise the serial interval\n    \n    # Initialise output matrices\n    R = zeros(N, T) # Matrix to store particle values\n    I = zeros(N, T) # Local infections\n    W = zeros(N, T) # Matrix to store model weights\n    \n    # Sample from initial distribution\n    R[:,1] = rand(Uniform(0, 10), N)\n    I[:,1] = rand.(opts[\"pI0\"], N)\n    \n    # Run the filter\n    for tt = 2:T\n        \n        # Project according to the state-space model\n        R[:,tt] = exp.(rand.(Normal.(log.(R[:,tt-1]), θ[1])))\n        \n        # Calculate force-of-infection and sample infections\n        Λ = sum(I[:,tt-1:-1:1]  .* opts[\"ω\"][1:tt-1]', dims=2)\n        I[:,tt] = rand.(Poisson.(R[:,tt] .* Λ))\n        \n        # Weight according to the observation model\n        r = 1/θ[2]\n        p = 1 ./ (1 .+ θ[2] * I[:,tt])\n        W[:,tt] = pdf.(NegativeBinomial.(r, p), Y.local[tt])\n        \n        # Resample\n        inds = wsample(1:N, W[:,tt], N; replace=true)\n        R[:, max(tt - L, 1):tt] = R[inds, max(tt - L, 1):tt]\n        I[:, max(tt - L, 1):tt] = I[inds, max(tt - L, 1):tt]\n        \n    end\n    \n    # Store output as three-dimensional array\n    X = zeros(N, T, 2)\n    X[:,:,1] = R\n    X[:,:,2] = I\n    \n    return(X, W)\n    \nend\n\n\nopts = Dict(\n\n# Bootstrap filter options\n\"T\" =&gt; size(Y, 1), # Number of time-steps\n\"N\" =&gt; 1000, # Number of particles\n\"L\" =&gt; 50, # Fixed-lag resampling length\n\"ω\" =&gt; pdf.(Gamma(2.36, 2.74), 1:100), # Serial interval\n\"pR0\" =&gt; Uniform(0, 10), # Prior on Rt at t = 0\n\"pI0\" =&gt; Dirac(1),\n\"predictiveValues\" =&gt; false, # Whether to calculate predictive cases\n\n# PMMH options\n\"nChains\" =&gt; 3, # Number of chains\n\"chunkSize\" =&gt; 100, # Number of iterations per chunk\n\"maxChunks\" =&gt; 50, # Maximum number of chunks\n\"maxRhat\" =&gt; 1.02,  # Stopping criterion: maximum Rhat value\n\"minESS\" =&gt; 200, # Stopping criterion: minimum effective sample size\n\"showChunkProgress\" =&gt; true, # Whether to show progress of each chunk\n\"propStdDevInit\" =&gt; sqrt.([0.1, 0.01]), # Initial proposal standard deviation (this is adaptively fit)\n\"paramPriors\" =&gt; [Uniform(0, 1), Uniform(0, 1)],\n\"initialParamSamplers\" =&gt; [Uniform(0.1, 0.3), Uniform(0.01, 0.03)],\n\"paramLimits\" =&gt; [(0, 1), (0, 1)],\n\"paramNames\" =&gt; [\"σ\", \"ϕ\"],\n\n# Marginal posterior options\n\"posteriorNumberOfParticles\" =&gt; 10000, # Number of particles to use for marginal posterior\n\"posteriorParamSamples\" =&gt; 100,\n\"stateNames\" =&gt; [\"Rt\", \"It\"]\n)\n\n\n\nfunction runVaryingL()\n    \n    Lvals = [5, 10, 20, 40, 80]\n    \n    df_states_all = DataFrame()\n    df_params_all = DataFrame()\n    \n    for L in Lvals\n        \n        println(\"L = \", L)\n        \n        # Update the options with the new L value\n        opts_in = deepcopy(opts)\n        opts_in[\"L\"] = L\n        \n        # Run the PMMH algorithm\n        (df_states, df_params, _, _) = fitModel(ExampleModel3, Y, opts_in; verbose=true)\n        df_states[!,:L] .= L\n        df_params[!,:L] .= L\n        \n        # Store\n        df_states_all = vcat(df_states_all, df_states)\n        df_params_all = vcat(df_params_all, df_params)\n        \n    end\n    \n    # Return the results\n    return(df_states_all, df_params_all)\n    \nend\n\n# # Run all models (this takes a few minutes) and save the results\n# (df_states_all, df_params_all) = runVaryingL()\n# CSV.write(\"results/df_states_all.csv\", df_states_all)\n# CSV.write(\"results/df_params_all.csv\", df_params_all)\n\n\nfunction plotVaryingL_animated()\n    df_states_all = CSV.read(\"results/df_states_all.csv\", DataFrame)\n    df_params_all = CSV.read(\"results/df_params_all.csv\", DataFrame)\n    Lvals = sort(unique(df_states_all.L))\n\n    dfsig = df_params_all[df_params_all.param .== \"σ\", :]\n    plot_sigma = plot(xlab=\"L\", ylab=\"σ\", xscale=:log2, xticks=(dfsig.L, string.(dfsig.L)), label=false, ylims=(0, 0.5), xlims=(4, 110))\n    plot_sigma = scatter!(dfsig.L, dfsig.m, yerror=(dfsig.m-dfsig.l, dfsig.u-dfsig.m), label=false)\n\n    dfphi = df_params_all[df_params_all.param .== \"ϕ\", :]\n    plot_phi = plot(xlab=\"L\", ylab=\"ϕ\", xscale=:log2, xticks=(dfphi.L, string.(dfphi.L)), label=false, ylims=(0, 0.1), xlims=(4, 110))\n    plot_phi = scatter!(dfphi.L, dfphi.m, yerror=(dfphi.m-dfphi.l, dfphi.u-dfphi.m), label=false)\n\n    anim = Animation()\nfor i in 1:length(Lvals)\n    plot_R = plot(xlab=\"Date\", ylab=\"Reproduction number\", label=false, legend=:topright, ylim=(0, 10), color_palette=:Dark2_8)\n    for (j, L) in enumerate(Lvals[1:i])\n        dfL = df_states_all[df_states_all.L .== L, :]\n        dfLR = dfL[dfL.variable .== \"Rt\", :]\n        plot_R = plot!(dfLR.date, mean.(dfLR.mean), ribbon=(dfLR.mean-dfLR.lower, dfLR.upper-dfLR.mean), label=\"L = $L\", color=palette(:Dark2_8)[j])\n    end\n    p = plot(plot(plot_sigma, plot_phi, layout=(1,2)), plot_R, layout=(2,1), size=(600,400), margins=3mm)\n    frame(anim, p)\nend\ngif(anim, \"varying_L.gif\", fps=1, show_msg=false)\n\nend\n\nplotVaryingL_animated()",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Putting it together</span>"
    ]
  },
  {
    "objectID": "smc-combining.html#footnotes",
    "href": "smc-combining.html#footnotes",
    "title": "6  Putting it together",
    "section": "",
    "text": "Many generation time distributions are assumed to have support on \\((0, \\infty)\\). In this case, \\(L\\) should be chosen such that it is greater than most of the possible generation times, such as the \\(99^{th}\\) percentile, for example.↩︎",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Putting it together</span>"
    ]
  },
  {
    "objectID": "smc-other.html",
    "href": "smc-other.html",
    "title": "7  Other resources",
    "section": "",
    "text": "7.1 Temfack and Wyse (2024)\nWe have outlined additional resources at the end of relevant sections. This chapter is specifically dedicated to papers presenting an overall framework in the style of a primer or tutorial.\nTemfack and Wyse (2024) provide an excellent introduction to SMC methods in epidemiology, albeit with a focus on SEIR-type compartmental models rather than the renewal model. They also target the filtering posterior distribution rather than the smoothing posterior distribuon. Despite these differences, many considerations in their paper are useful in our context too. To assist with interpretation, we contextualise and compare their work here.\nAfter providing a useful background of SMC methods in epidemiology (section 2), and an introduction to the discrete-time stochastic SEIR model (section 3), they describe a general state-space model (abbreviated as SSM) in section 3.1 as:\n\\[\n\\begin{aligned}\n& x_0 \\sim f(x_0|\\theta) & \\text{Initial state} \\\\\n& x_t | x_{t-1} \\sim f(x_t|x_{t-1}, \\theta) & \\text{State process} \\\\\n& y_t | x_t \\sim g(y_t | x_t, \\theta) & \\text{Observation process}\n\\end{aligned}\n\\]\nTheir state process corresponds to our state-space transition distribution, while their observation process corresponds to our observation distribution.\nThis construction is a special case of ours for two reasons:\nThe Markovian structure allows for a much wider range of SMC-type methods (often more efficient than ours) to be used, at the cost of a restricted range of models that can be fit. The renewal model is inherently non-Markovian, so cannot be fit within their framework.\nThe authors then introduce general sequential importance sampling (section 3.1.1) and, like us, use the state process as the proposal density, noting that this is known to be suboptimal. Particle degeneracy is tackled via resampling (section 3.1.2), although they resample only when the effective sample size of their particles is below some threshold. This reduces computational load and decreases the impact of particle degeneracy but is incompatible with fixed-lag smoothing - their method returns samples from the filtering posterior distributions only.\nRather than PMMH, the authors augment the parameters \\(\\theta\\) as additional time-varying hidden states. To avoid severe particle degeneracy issues and the overestimation of the posterior variance of \\(\\theta\\), they use the approach of #TODO: cite liuCombinedParameterState2001. This approach allows the authors to handle the estimation of \\(\\theta\\) alongside \\(X_t\\) (thus all inference can be performed in a single algorithm), but this approach is again incompatible with our fixed-lag smoothing.",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Other resources</span>"
    ]
  },
  {
    "objectID": "smc-other.html#temfack-and-wyse-2024",
    "href": "smc-other.html#temfack-and-wyse-2024",
    "title": "7  Other resources",
    "section": "",
    "text": "Their state process is explicitly Markovian: \\(f\\) depends only on \\(x_{t-1}\\), whereas our state-space transition distribution can depend on all past values \\(x_{1:t-1}\\).\nTheir observation process depends only on the current hidden state \\(x_t\\). Our observation distribution can depend on past values of both the hidden state \\(x_{1:t}\\) and the observed data \\(y_{1:t-1}\\).",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Other resources</span>"
    ]
  },
  {
    "objectID": "smc-other.html#endo-van-leeuwen-and-baguelin-2019",
    "href": "smc-other.html#endo-van-leeuwen-and-baguelin-2019",
    "title": "7  Other resources",
    "section": "7.2 Endo, van Leeuwen, and Baguelin (2019)",
    "text": "7.2 Endo, van Leeuwen, and Baguelin (2019)\nOur overall approach is similar to Endo, van Leeuwen, and Baguelin (2019), although there remain some crucial differences. Before getting into details, we highlight that their figure 3 provides an excellent graphic overview of the SMC algorithm.\nThe underlying model fit in Endo, van Leeuwen, and Baguelin (2019) is defined in the same fashion as Temfack and Wyse (2024), including using \\(f\\) and \\(g\\) to represent the same state and observation processes respectively. In this case, they do not explicitly write the dependence on model parameters \\(\\theta\\), although highlight in the text that both \\(f\\) and \\(g\\) can depend on \\(\\theta\\) implicitly.\nLike other papers in the field, Endo, van Leeuwen, and Baguelin (2019) focus on methods for Markovian models with applications to compartmental SIR-type epidemic models. They focus on “Particle Markov-chain Monte Carlo” (PMCMC), which is largely analagous to the PMMH algorithm we use (Section 5.3). Like PMMH, PMCMC uses a particle filter to estimate the likelihood of proposed parameter vectors.\nUnlike our approach, however, the authors do not separate the inference of \\(\\theta\\) from the inference of \\(X_t\\). To recap, in our approach we:\n\nUse PMMH to sample from \\(P(\\theta|y_{1:T})\\), using the bootstrap filter as a subroutine.\nRun the bootstrap filter at a sufficiently large number of samples of \\(\\theta\\), storing all particle trajectories.\n\nThis results in waste: we run many bootstrap filters in step (a), the results of which are lost.\nIn contrast, Endo, van Leeuwen, and Baguelin (2019) only perform the equivalent of step (a). They retain a single trajectory \\(x_{1:T}\\) from each accepted parameter sample, generating a set \\(\\{x_{1:T}^{(i)}, \\theta^{(i)}\\}_{i=1}^M\\). This uses information from every bootstrap filter that was run at an accepted value of \\(\\theta\\).\nWhile this appears more computationally efficient, in order to effectively pre-allocate memory to store the (possibly very high dimensional) particle trajectories, we would need to know at what value of \\(M\\) to stop the PMMH algorithm. We instead prioritise the adaptivity of our approach over the possible efficiency gains of Endo, van Leeuwen, and Baguelin (2019).\nDespite this, the overall framework of Endo, van Leeuwen, and Baguelin (2019) could be adapted to fit non-Markovian models by replacing their bootstrap filter with the one outlined in Chapter 4.",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Other resources</span>"
    ]
  },
  {
    "objectID": "smc-other.html#yang-et-al.-2022",
    "href": "smc-other.html#yang-et-al.-2022",
    "title": "7  Other resources",
    "section": "7.3 Yang et al. (2022)",
    "text": "7.3 Yang et al. (2022)\nYang et al. (2022) introduce “Bayesian data assimilation” for \\(R_t\\) estimation. Like our methods (and unlike those listed above), they introduce methods that can handle non-Markovian models, and thus are able to apply their methods to the renewal model.\nHowever, their paper and supplementary material do not contain sufficient detail to fully understand their approach. They claim to use a particle approximation to the forward filtering and backward smoothing equations separately, which is a different approach to ours (where smoothing is performed while filtering through fixed-lag resampling). Details on the specific particle filter used are scarce.\nYang et al. (2022) also do not provide methods for the estimation of fixed parameters \\(\\theta\\). Their methods are an alternative to the bootstrap filter (Chapter 4) but not to PMMH (Chapter 5).\n\n\n\n\nEndo, Akira, Edwin van Leeuwen, and Marc Baguelin. 2019. “Introduction to Particle Markov-chain Monte Carlo for Disease Dynamics Modellers.” Epidemics 29 (December): 100363. https://doi.org/10.1016/j.epidem.2019.100363.\n\n\nTemfack, Dhorasso, and Jason Wyse. 2024. “A Review of Sequential Monte Carlo Methods for Real-Time Disease Modeling.” arXiv. https://doi.org/10.48550/ARXIV.2408.15739.\n\n\nYang, Xian, Shuo Wang, Yuting Xing, Ling Li, Richard Yi Da Xu, Karl J. Friston, and Yike Guo. 2022. “Bayesian Data Assimilation for Estimating Instantaneous Reproduction Numbers During Epidemics: Applications to COVID-19.” PLOS Computational Biology 18 (2): e1009807. https://doi.org/10.1371/journal.pcbi.1009807.",
    "crumbs": [
      "Sequential Monte Carlo",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Other resources</span>"
    ]
  },
  {
    "objectID": "models-simple.html",
    "href": "models-simple.html",
    "title": "8  Introductory models",
    "section": "",
    "text": "8.1 Example 1: indepdendent daily estimates\nWe begin by fitting the three example models introduced in Section 3.2 of Chapter 3 to real-world data.\nFirst we load and plot the data we will use for our examples. These are the national reported cases for the first 100 days of the COVID-19 pandemic in Aotearoa New Zealand (Ministry of Health NZ 2024), downloaded from here.\nIn this example, we use a Gamma\\((1, 0.2)\\) prior distribution for \\(R_t\\) which forms the state-space transition distribution:\n\\[\nR_t \\sim \\text{Gamma}(1, 1/0.2)\n\\]\nand use the Poisson renewal model (with a Gamma(2.36, 27.4) distribution for the serial interval) for the observation distribution:\n\\[\nC_t | R_t, C_{1:t-1} \\sim \\text{Poisson}\\left(R_t \\sum_{u=1}^{u_{max}} C_{t-u} \\omega_u\\right)\n\\]\nWe write this as a hidden-state model in Julia:\nfunction ExampleModel1(θ, Y::DataFrame, opts::Dict)\n\n    # Extract frequently used options\n    T = opts[\"T\"] # Number of time steps\n    N = opts[\"N\"] # Number of particles to use\n\n    # Define the serial interval\n    ω = pdf.(Gamma(2.36, 2.74), 1:100) # (Unnormalised) serial interval\n    ω = ω/sum(ω) # Normalise the serial interval\n\n    # Initialise output matrices\n    R = zeros(N, T) # Matrix to store particle values\n    W = zeros(N, T) # Matrix to store model weights\n\n    # Run the filter\n    for tt = 2:T\n\n        # \"Project\" according to the state-space model\n        R[:,tt] = rand(Gamma.(1, 1/0.2), N)\n\n        # Weight according to the observation model\n        Λ = sum(Y.Ct[tt-1:-1:1] .* ω[1:tt-1])/sum(ω[1:tt-1]) # Calculate the force-of-infection\n        W[:,tt] = pdf.(Poisson.(R[:,tt] .* Λ), Y.Ct[tt])\n\n        # Resample\n        inds = wsample(1:N, W[:,tt], N; replace=true)\n        R[:, tt] = R[inds, tt]\n\n    end\n\n    return(R, W)\n\nend\nThere are no parameters to estimate, so we simply fit this model to the data:\n# Specify bootstrap filter options (the {String, Any} term allows us to use any type of value in this dictionary)\nopts = Dict{String, Any}(\"N\" =&gt; 1000, \"T\" =&gt; 100)\n\n# Fit the model\n(R, W) = ExampleModel1(missing, Y, opts) # θ is missing as there are no \"parameters\" in this model!\n\n# Extract posterior means and credible intervals\nMeanRt = vec(mean(R, dims=1)) # \"dims=1\" tells Julia to take column-wise means, vec(.) turns the resulting 1xN matrix into a vector\nLowerRt = [quantile(Rt, 0.025) for Rt in eachcol(R)]\nUpperRt = [quantile(Rt, 0.975) for Rt in eachcol(R)]\nWe can also find the posterior means and credible intervals for this model analytically (Section 1.2), useful for checking our algorithm.\nCode\n(MeanRtTrue, LowerRtTrue, UpperRtTrue) = (zeros(opts[\"T\"]), zeros(opts[\"T\"]), zeros(opts[\"T\"])) # Pre-allocate results vectors\n(a0, b0) = (1, 1/5) # Set prior parameters\n\n# Define the serial interval\nω = pdf.(Gamma(2.36, 2.74), 1:100) # (Unnormalised) serial interval\nω = ω/sum(ω) # Normalise the serial interval\n\nfor tt = 2:opts[\"T\"]\n\n    # Find the posterior distribution on day t\n    a = a0 + Y.Ct[tt]\n    b = b0 + sum(Y.Ct[tt-1:-1:1] .* ω[1:tt-1])/sum(ω[1:tt-1])\n    PosteriorDist = Gamma(a, 1/b)\n\n    # Save the results\n    MeanRtTrue[tt] = mean(PosteriorDist)\n    LowerRtTrue[tt] = quantile(PosteriorDist, 0.025)\n    UpperRtTrue[tt] = quantile(PosteriorDist, 0.975)\n\nend\nPlotting the estimates from the bootstrap filter against the analytical posterior mean and credible intervals demonstrates the SMC approxmation is high quality.\n# Process the results and plot\npltR = plot(size=(800,400), xlabel=\"Date\", ylabel=\"Reproduction number\", margins=3mm)\npltR = plot!(pltR, Y.date, MeanRtTrue, ribbon=(MeanRtTrue-LowerRtTrue, UpperRtTrue-MeanRtTrue), color=:lightgreen, label=\"Rt (analytical)\")\npltR = plot!(pltR, Y.date, MeanRt, ribbon=(MeanRt-LowerRt, UpperRt-MeanRt), color=:darkgreen, label=\"Rt (from SMC)\")\nhline!([1], color=:black, linestyle=:dash, label=\"Rt = 1\")\n\n\n\n\n\n\n\nFigure 8.2: Reproduction number estimates from example model 1 fit to data from the first 100 days of COVID-19 in Aotearoa New Zealand. N = 1000 particles are used in the SMC approxmation, resulting in a very good approximation to the analytical posterior means and credible intervals over time.",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introductory models</span>"
    ]
  },
  {
    "objectID": "models-simple.html#example-1-indepdendent-daily-estimates",
    "href": "models-simple.html#example-1-indepdendent-daily-estimates",
    "title": "8  Introductory models",
    "section": "",
    "text": "Tip\n\n\n\nAs there is no autocorrelation in the state-space model, and the observation distributon depends only on the hidden-states at time \\(t\\), we do not need to bother with any fixed-lag resampling.",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introductory models</span>"
    ]
  },
  {
    "objectID": "models-simple.html#example-2-temporally-smoothed-estimates",
    "href": "models-simple.html#example-2-temporally-smoothed-estimates",
    "title": "8  Introductory models",
    "section": "8.2 Example 2: temporally smoothed estimates",
    "text": "8.2 Example 2: temporally smoothed estimates\nFigure 8.2 demonstrates considerable variability in \\(R_t\\) estimates. In pratice, we want to produce smoothed estimates (Section 1.2.2).\nWe update the state-space model to use a Gaussian random walk on \\(\\log R_t\\):\n\\[\n\\log R_t | \\log R_{t-1} \\sim \\text{Normal}(\\log R_{t-1}, \\sigma)\n\\]\nand retain the same observation model:\n\\[\nC_t | R_t, C_{1:t-1} \\sim \\text{Poisson}\\left(R_t \\sum_{u=1}^{u_{max}} C_{t-u} \\omega_u\\right)\n\\]\nIn Julia:\n\nfunction ExampleModel2(θ, Y::DataFrame, opts::Dict)\n\n    # Extract frequently used options\n    T = opts[\"T\"] # Number of time steps\n    N = opts[\"N\"] # Number of particles to use\n    L = opts[\"L\"] # Length of fixed-lag resampling\n\n    # Define the serial interval\n    ω = pdf.(Gamma(2.36, 2.74), 1:100) # (Unnormalised) serial interval\n    ω = ω/sum(ω) # Normalise the serial interval\n\n    # Initialise output matrices\n    R = zeros(N, T) # Matrix to store particle values\n    W = zeros(N, T) # Matrix to store model weights\n\n    # Sample from initial distribution\n    R[:,1] = rand(Uniform(0, 10), N)\n\n    # Run the filter\n    for tt = 2:T\n\n        # Project according to the state-space model\n        R[:,tt] = exp.(rand.(Normal.(log.(R[:,tt-1]), θ[1])))\n\n        # Weight according to the observation model\n        Λ = sum(Y.Ct[tt-1:-1:1] .* ω[1:tt-1])/sum(ω[1:tt-1]) # Calculate the force-of-infection\n        W[:,tt] = pdf.(Poisson.(R[:,tt] .* Λ), Y.Ct[tt])\n\n        # Resample\n        inds = wsample(1:N, W[:,tt], N; replace=true)\n        R[:, max(tt - L, 1):tt] = R[inds, max(tt - L, 1):tt]\n\n    end\n\n    return(R, W)\n\nend\n\nNow we have parameter \\(\\sigma\\) to estimate. We use PMMH to do this. First, we define our model-fitting options:\n\nopts = Dict(\n\n    # Bootstrap filter options\n    \"T\" =&gt; size(Y, 1), # Number of time-steps\n    \"N\" =&gt; 1000, # Number of particles\n    \"L\" =&gt; 50, # Fixed-lag resampling length\n    \"ω\" =&gt; pdf.(Gamma(2.36, 2.74), 1:100), # Serial interval\n    \"pR0\" =&gt; Uniform(0, 10), # Prior on Rt at t = 0\n    \"predictiveValues\" =&gt; false, # Whether to calculate predictive cases\n\n    # PMMH options\n    \"nChains\" =&gt; 3, # Number of chains\n    \"chunkSize\" =&gt; 100, # Number of iterations per chunk\n    \"maxChunks\" =&gt; 50, # Maximum number of chunks\n    \"maxRhat\" =&gt; 1.05,  # Stopping criterion: maximum Rhat value\n    \"minESS\" =&gt; 100, # Stopping criterion: minimum effective sample size\n    \"showChunkProgress\" =&gt; true, # Whether to show progress of each chunk\n    \"propStdDevInit\" =&gt; [0.1], # Initial proposal standard deviation (this is adaptively fit)\n    \"paramPriors\" =&gt; [Uniform(0, 1)],\n    \"initialParamSamplers\" =&gt; [Uniform(0.1, 0.3)],\n    \"paramLimits\" =&gt; [(0, 1)],\n    \"paramNames\" =&gt; [\"σ\"]\n);\n\nAnd the PMMH() function handles the rest:\n\ninclude(\"../src/PMMH.jl\")\n\n(θ, diag) = PMMH(ExampleModel2, Y, opts; verbose=false) #  Set verbose = true to see \n\nChains(θ, opts[\"paramNames\"])\n\n\nChains MCMC chain (200×1×3 Array{Float64, 3}):\nIterations        = 1:1:200\nNumber of chains  = 3\nSamples per chain = 200\nparameters        = σ\nSummary Statistics\n  parameters      mean       std   naive_se      mcse        ess      rhat \n      Symbol   Float64   Float64    Float64   Float64    Float64   Float64 \n           σ    0.2462    0.0493     0.0020    0.0049   113.5474    1.0251\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5% \n      Symbol   Float64   Float64   Float64   Float64   Float64 \n           σ    0.1661    0.2142    0.2347    0.2823    0.3590\n\n\n\n\nWhich tells us \\(\\sigma = 0.24 \\ (0.17, 0.33)\\).\nIn isolation, this isn’t a particularly useful quantity. We actually care about the estimates of \\(R_t\\), ideally after accouting for uncertainty about \\(\\sigma\\). We do this by marginalising out this parameter - this is performed by the marginalPosterior() function.\nFirst, we need to specify two more options, to tell the function to run the bootstrap filter at 100 samples of \\(\\sigma\\), for 1,000 particles each (a total of 100,000 particles).\n\nopts[\"posteriorNumberOfParticles\"] = 1000;\nopts[\"posteriorParamSamples\"] = 100;\n\nWe can then call the marginalPosterior() function, also processing and plotting the output:\n\ninclude(\"../src/MarginalPosterior.jl\")\n\nR = marginalPosterior(ExampleModel2, θ, Y, opts; showProgress=true)\n\n(MeanRt2, MedianRt2, LowerRt2, UpperRt2) = processResults(R) # This is a helper function which calculates columnwise summary statistics\n\npltR2 = plot(size=(800,400), xlabel=\"Date\", ylabel=\"Reproduction number\", margins=3mm)\npltR2 = plot!(pltR2, Y.date, MeanRt2, ribbon=(MeanRt2-LowerRt2, UpperRt2-MeanRt2), color=:darkgreen, label=false)\nhline!([1], color=:black, linestyle=:dash, label=\"Rt = 1\")\n\n\n\n\n\n\n\n\nWhich demonstrates much smoother estimates of \\(R_t\\). However, this model does not allow for observation noise, other than in the stochasticity in transmission (the Poisson renewal model). In practice, reported cases are noisy observations of true infections - the final example accounts for this.",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introductory models</span>"
    ]
  },
  {
    "objectID": "models-simple.html#sec-models-simple-example3",
    "href": "models-simple.html#sec-models-simple-example3",
    "title": "8  Introductory models",
    "section": "8.3 Example 3: with observation noise",
    "text": "8.3 Example 3: with observation noise\nTo allow for observation noise, we now employ the Poisson renewal model in the state-space model, using \\(I_t\\) to represent unobserved infections:\n\\[\n\\log R_t | \\log R_{t-1} \\sim \\text{Normal}(\\log R_{t-1}, \\sigma)\n\\]\n\\[\nI_t | R_t, I_{1:t-1} \\sim \\text{Poisson}\\left(R_t \\sum_{u=1}^{u_{max}} I_{t-u} \\omega_u\\right)\n\\]\nand assume that reported cases follow a negative binomial distribution with mean \\(I_t\\) and variance \\(I_t + \\phi I_t^2\\) (the observation model):\n\\[\nC_t | I_t \\sim \\text{Negative Binomial}\\left(r=\\frac{1}{\\phi}, p=\\frac{1}{1+\\phi I_t} \\right)\n\\]\nFor simplicity, we use the same PMF for \\(\\omega_u\\) here, even though it now represents the generation time distribution (time from infection-to-infection), rather than the serial interval (time from reporting-to-reporting).\nWe have two fixed parameters: \\(\\sigma\\) and \\(\\phi\\), with the latter controlling the level of observation noise.\nWriting this as a bootstrap filter in Julia:\n\nfunction ExampleModel3(θ, Y::DataFrame, opts::Dict)\n\n    # Extract frequently used options\n    T = opts[\"T\"] # Number of time steps\n    N = opts[\"N\"] # Number of particles to use\n    L = opts[\"L\"] # Length of fixed-lag resampling\n\n    # Define the serial interval\n    ω = pdf.(Gamma(2.36, 2.74), 1:100) # (Unnormalised) serial interval\n    ω = ω/sum(ω) # Normalise the serial interval\n\n    # Initialise output matrices\n    R = zeros(N, T) # Matrix to store particle values\n    I = zeros(N, T) # Local infections\n    W = zeros(N, T) # Matrix to store model weights\n\n    # Sample from initial distribution\n    R[:,1] = rand(Uniform(0, 10), N)\n    I[:,1] = rand.(opts[\"pI0\"], N)\n\n    # Run the filter\n    for tt = 2:T\n\n    # Project according to the state-space model\n        R[:,tt] = exp.(rand.(Normal.(log.(R[:,tt-1]), θ[1])))\n        \n        # Calculate force-of-infection and sample infections\n        Λ = sum(I[:,tt-1:-1:1]  .* opts[\"ω\"][1:tt-1]', dims=2)\n        I[:,tt] = rand.(Poisson.(R[:,tt] .* Λ))\n\n        # Weight according to the observation model\n        r = 1/θ[2]\n        p = 1 ./ (1 .+ θ[2] * I[:,tt])\n        W[:,tt] = pdf.(NegativeBinomial.(r, p), Y.local[tt])\n\n        # Resample\n        inds = wsample(1:N, W[:,tt], N; replace=true)\n        R[:, max(tt - L, 1):tt] = R[inds, max(tt - L, 1):tt]\n        I[:, max(tt - L, 1):tt] = I[inds, max(tt - L, 1):tt]\n\n    end\n\n    # Store output as three-dimensional array\n    X = zeros(N, T, 2)\n    X[:,:,1] = R\n    X[:,:,2] = I\n\n    return(X, W)\n\nend\n\nThis time we use fitModel() function to perform parameter inference and marginalistion in a single step. We also update some PMMH options to reflect the additional parameter being estimated.\n\ninclude(\"../src/FitModel.jl\")\n\nopts[\"pI0\"] = Dirac(1) # Initial distiribution for It at t = 0, we assume one case to start the epidemic\n\nopts[\"propStdDevInit\"] = sqrt.([0.1, 0.01])\nopts[\"paramPriors\"] = [Uniform(0, 1), Uniform(0, 1)]\nopts[\"initialParamSamplers\"] = [Uniform(0.1, 0.3), Uniform(0.01, 0.03)]\nopts[\"paramLimits\"] = [(0, 1), (0, 1)]\nopts[\"paramNames\"] = [\"σ\", \"ϕ\"]\n\nopts[\"stateNames\"] = [\"Rt\", \"It\"]\n(df_states, df_params, θ, diag) = fitModel(ExampleModel3, Y, opts; skipResamplingPMMH=false);\n\nInterrogating the output, we first check the parameter estimates:\n\ndisplay(df_params)\n\n2 rows × 6 columnsparammlurhatessStringFloat64Float64Float64Float64Float641σ0.2284920.1421130.365931.04315126.7782ϕ0.01892560.0002398320.07419121.024111.536\n\n\nAnd plot the reproduction number estimates:\n\ndf_Rt  = df_states[df_states.variable.==\"Rt\",:]\ndf_It = df_states[df_states.variable.==\"It\",:]\n\npltR3 = plot(size=(800,400), xlabel=\"Date\", ylabel=\"Reproduction number\", margins=3mm)\npltR3 = plot!(pltR3, df_Rt.date, df_Rt.mean, ribbon=(df_Rt.mean-df_Rt.lower, df_Rt.upper-df_Rt.mean), color=:darkgreen, label=false)\npltR3 = hline!([1], color=:black, linestyle=:dash, label=\"Rt = 1\")\n\n\n\n\n\n\n\n\nShowing even smoother estimates of \\(R_t\\). In example 2, all unexpected variation in case counts must be attributed to changes in the reproduction number, whereas this model allows for noise in the case data themselves.\nThese still aren’t the best possible estimates, however! In this example, many of the reported cases were infected overseas - we don’t account for these here, hence these \\(R_t\\) estimates are biased upward.\n\n\n\n\nMinistry of Health NZ. 2024. “New Zealand COVID-19 Data.”",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Introductory models</span>"
    ]
  },
  {
    "objectID": "models_obsnoise.html",
    "href": "models_obsnoise.html",
    "title": "9  Modelling observation noise",
    "section": "",
    "text": "9.1 Underreporting\nObservation noise is frequently highlighted as a key concern when fitting models to epidemiological data. We saw the impact of ignoring observation noise in Chapter 8. In this chapter, we examine a few possible models for obesrvation noise.\nAll five models in this chapter use the same state-space model:\n\\[\n\\begin{align}\n\\log R_t | \\log R_{t-1} &\\sim \\text{Normal}(\\log R_{t-1}, \\sigma) \\\\\nI_t | R_t, I_{1:t-1} &\\sim \\text{Poisson}(R_t \\Lambda_t)\n\\end{align}\n\\tag{9.1}\\]\nand differ only in the observation model.\nIf we assume that each infected individual has an independent probability of being reported as a case, we can model \\(C_t\\) using a binomial distributon: \\[\nC_t | I_t \\sim \\text{Binomial}\\left(I_t, \\ \\rho\\right)\n\\tag{9.2}\\]\nThe reporting rate \\(\\rho\\) is not identifiable from reported case data alone, so must be set by the user. We use \\(\\rho = 0.5\\) in this example. This model has the same number of fixed parameters as the basic model (one parameter, \\(\\sigma\\)). A Julia implementation of this hidden-state model is provided in the following drop-down box:\nCode\nfunction underreportingModel(σ, Y::DataFrame, opts::Dict)\n\n    # Specify reporting rate\n    ρ = 0.5\n\n    # Extract frequently used options\n    T = opts[\"T\"]\n    N = opts[\"N\"]\n    L = opts[\"L\"]\n    ω = opts[\"ω\"]\n\n    # Initialise output matrices\n    R = zeros(N, T)\n    I = zeros(N, T)\n    W = zeros(N, T)\n\n    # Sample from initial distributions\n    R[:,1] = rand.(opts[\"pR0\"], N)\n    I[:,1] = rand.(opts[\"pI0\"], N)\n\n    # Run the filter\n    for tt = 2:T\n\n        # Project according to the state-space model\n        R[:,tt] = exp.(rand.(Normal.(log.(R[:,tt-1]), σ)))\n        Λ = sum(I[:, (tt-1):-1:1] .* ω[1:(tt-1)]', dims=2)\n        I[:,tt] = rand.(Poisson.(R[:,tt] .* Λ))\n\n        # Weight according to the observation model\n        W[:,tt] = pdf.(Binomial.(I[:,tt], ρ), Y.Ct[tt])\n\n        # Resample\n        inds = wsample(1:N, W[:,tt], N; replace=true)\n        R[:, max(tt - L, 1):tt] = R[inds, max(tt - L, 1):tt]\n        I[:, max(tt - L, 1):tt] = I[inds, max(tt - L, 1):tt]\n\n    end\n\n    # Store output as three-dimensional array\n    X = zeros(N, T, 2)\n    X[:,:,1] = R\n    X[:,:,2] = I\n\n    return(X, W)\n\nend\nwe also specify the standard options dictionary:\nCode\nopts = Dict(\n\n    # Bootstrap filter options\n    \"T\" =&gt; 100, # Number of time-steps\n    \"N\" =&gt; 1000, # Number of particles\n    \"L\" =&gt; 50, # Fixed-lag resampling length\n    \"pR0\" =&gt; Uniform(0, 10), # Prior on Rt at t = 0\n    \"pI0\" =&gt; DiscreteUniform(1, 5), # Prior on It at t = 0\n    \"ω\" =&gt; pdf.(Gamma(2.36, 2.74), 1:100) / sum(pdf.(Gamma(2.36, 2.74), 1:100)), # Serial interval\n\n    # PMMH options\n    \"nChains\" =&gt; 3,\n    \"chunkSize\" =&gt; 100,\n    \"maxChunks\" =&gt; 50,\n    \"maxRhat\" =&gt; 1.05,\n    \"minESS\" =&gt; 100,\n    \"showChunkProgress\" =&gt; false,\n    \"propStdDevInit\" =&gt; [0.1],\n    \"paramPriors\" =&gt; [Uniform(0, 1)],\n    \"initialParamSamplers\" =&gt; [Uniform(0.1, 0.3)],\n    \"paramNames\" =&gt; [\"σ\"],\n\n    # Marginal posterior options\n    \"posteriorNumberOfParticles\" =&gt; 1000,\n    \"posteriorParamSamples\" =&gt; 100,\n    \"stateNames\" =&gt; [\"Rt\", \"It\"]\n);\nNow we can use fitModel() to run the PMMH algorithm and return the marginalised estimates of \\(R_t\\). We fit this model to the same data used in Chapter 8.\n(df_states, df_params, θ, diag) = fitModel(underreportingModel, Y, opts)\nFinally, we extract the hidden-state estimates (both \\(R_t\\) and \\(I_t\\)) and plot them:\nCode\ndf_R = df_states[df_states.variable .== \"Rt\",:]\ndf_I = df_states[df_states.variable .== \"It\",:]\n\nplotR = plot(df_R.date, df_R.mean, ribbon=(df_R.mean-df_R.lower, df_R.upper-df_R.mean), label=false, color=:darkgreen, xlabel=\"Date\", ylabel=\"Reproduction number\")\nplotI = scatter(df_I.date, df_I.mean, yerror=(df_I.mean-df_I.lower, df_I.upper-df_I.mean), label=false, color=:darkgreen, xlabel=\"Date\", ylabel=\"Infection incidence\", markersize=2)\nplot(plotR, plotI, layout=(2,1), size=(800,400), margins=3mm)\n\n\n\n\n\nReproduction number and infection incidence estimates from fitting a binomial observation model to data from the first 100 days of the COVID-19 pandemic in New Zealand. A fixed reporting rate of \\(\\rho = 0.5\\) is assumed - results are heavily dependent on this assumption.",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelling observation noise</span>"
    ]
  },
  {
    "objectID": "models_obsnoise.html#sec-models-obsnoise-underreportandover",
    "href": "models_obsnoise.html#sec-models-obsnoise-underreportandover",
    "title": "9  Modelling observation noise",
    "section": "9.2 Underreporting and overdispersion",
    "text": "9.2 Underreporting and overdispersion\nCase reporting can often be over-dispersed, exhibiting greater variance than implied by the binomial observation distribution above. Instead, we can use a beta-binomial distribution:\n\\[\nC_t | I_t \\sim \\text{Beta-binomial}\\left(I_t, \\alpha_t, \\beta_t\\right)\n\\tag{9.3}\\]\nwhere:\n\\[\n\\alpha_t = \\rho \\left(\\frac{1}{\\phi} - 1\\right), \\ \\ \\beta_t = (1-\\rho) \\left(\\frac{1}{\\phi} - 1\\right)\n\\tag{9.4}\\]\nThis model also assumes a reporting rate of \\(\\rho\\), but allows for additional variance through parameter \\(\\phi\\)1. We now must estimate two parameters: \\(\\sigma\\) and \\(\\phi\\).\n\n\nCode\nfunction underreportedAndOverdispersedModel(θ, Y::DataFrame, opts::Dict)\n\n    # Specify reporting rate\n    ρ = 0.5\n\n    # Extract frequently used options\n    T = opts[\"T\"]\n    N = opts[\"N\"]\n    L = opts[\"L\"]\n    ω = opts[\"ω\"]\n\n    # Initialise output matrices\n    R = zeros(N, T)\n    I = zeros(N, T)\n    W = zeros(N, T)\n\n    # Sample from initial distributions\n    R[:,1] = rand.(opts[\"pR0\"], N)\n    I[:,1] = rand.(opts[\"pI0\"], N)\n\n    # Run the filter\n    for tt = 2:T\n\n        # Project according to the state-space model\n        R[:,tt] = exp.(rand.(Normal.(log.(R[:,tt-1]), θ[1])))\n        Λ = sum(I[:, (tt-1):-1:1] .* ω[1:(tt-1)]', dims=2)\n        I[:,tt] = rand.(Poisson.(R[:,tt] .* Λ))\n\n        # Weight according to the observation model\n        α = ρ * (1/θ[2] - 1)\n        β = (1 - ρ) * (1/θ[2] - 1)\n        W[:,tt] = pdf.(BetaBinomial.(I[:,tt], α, β), Y.Ct[tt])\n\n        # Resample\n        inds = wsample(1:N, W[:,tt], N; replace=true)\n        R[:, max(tt - L, 1):tt] = R[inds, max(tt - L, 1):tt]\n        I[:, max(tt - L, 1):tt] = I[inds, max(tt - L, 1):tt]\n\n    end\n\n    # Store output as three-dimensional array\n    X = zeros(N, T, 2)\n    X[:,:,1] = R\n    X[:,:,2] = I\n\n    return(X, W)\n\nend\n\nopts = Dict(\n\n    # Bootstrap filter options\n    \"T\" =&gt; 100, # Number of time-steps\n    \"N\" =&gt; 1000, # Number of particles\n    \"L\" =&gt; 50, # Fixed-lag resampling length\n    \"pR0\" =&gt; Uniform(0, 10), # Prior on Rt at t = 0\n    \"pI0\" =&gt; DiscreteUniform(1, 5), # Prior on It at t = 0\n    \"ω\" =&gt; pdf.(Gamma(2.36, 2.74), 1:100) / sum(pdf.(Gamma(2.36, 2.74), 1:100)), # Serial interval\n\n    # PMMH options\n    \"nChains\" =&gt; 3,\n    \"chunkSize\" =&gt; 100,\n    \"maxChunks\" =&gt; 50,\n    \"maxRhat\" =&gt; 1.05,\n    \"minESS\" =&gt; 100,\n    \"showChunkProgress\" =&gt; false,\n    \"propStdDevInit\" =&gt; [0.05, 0.01],\n    \"paramPriors\" =&gt; [Uniform(0, 1), Uniform(0, 1)],\n    \"initialParamSamplers\" =&gt; [Uniform(0.1, 0.3), Uniform(0.01, 0.02)],\n    \"paramNames\" =&gt; [\"σ\", \"ϕ\"],\n\n    # Marginal posterior options\n    \"posteriorNumberOfParticles\" =&gt; 5000,\n    \"posteriorParamSamples\" =&gt; 100,\n    \"stateNames\" =&gt; [\"Rt\", \"It\"]\n)\n\n(df_states, df_params, θ, diag) = fitModel(underreportedAndOverdispersedModel, Y, opts; verbose=false);\n\n\nInvestigating the parameter estimates and plotting the samples of \\(\\phi\\) suggest that overdispersion may be present, although we cannot rule out binomial noise (\\(\\phi = 0\\)), conditional on \\(\\rho = 0.5\\).\n\nθl = resizeParams(θ)\nplt = histogram(θl[:,2], bins=50, xlabel=\"ϕ\", ylabel=\"Proportion of samples\", color=:darkorange, legend=false, normalize=:probability, size = (600,400), margins=3mm)\n\n\n\n\n\n\n\n\nEstimates of \\(R_t\\) are similar, but slightly smoother, than the binomial model above.\n\n\nCode\ndf_R = df_states[df_states.variable .== \"Rt\",:]\ndf_I = df_states[df_states.variable .== \"It\",:]\n\nplotR = plot(df_R.date, df_R.mean, ribbon=(df_R.mean-df_R.lower, df_R.upper-df_R.mean), label=false, color=:darkgreen, xlabel=\"Date\", ylabel=\"Reproduction number\")\nplotI = scatter(df_I.date, df_I.mean, yerror=(df_I.mean-df_I.lower, df_I.upper-df_I.mean), label=false, color=:darkgreen, xlabel=\"Date\", ylabel=\"Infection incidence\", markersize=2)\nplot(plotR, plotI, layout=(2,1), size=(800,400), margins=3mm)",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelling observation noise</span>"
    ]
  },
  {
    "objectID": "models_obsnoise.html#observation-noise-without-underreporting",
    "href": "models_obsnoise.html#observation-noise-without-underreporting",
    "title": "9  Modelling observation noise",
    "section": "9.3 Observation noise without underreporting",
    "text": "9.3 Observation noise without underreporting\nIn Section 9.1, we assumed a pre-determined reporting rate \\(\\rho\\). Without additional information this parameter is not identifiable. If we do not want to assume a value of \\(\\rho\\), a popular distribution for modelling (potentially) overdispersed data is the negative binomial:\n\\[\nC_t | I_t \\sim \\text{Negative binomial}\\left(r = \\frac{I_t}{k}, p=\\frac{1}{1 + k} \\right)\n\\]\nwhich has mean \\(I_t\\) and variance \\((1+k) I_t\\), where \\(k\\) is a dispersion parameter. This results in two parameters to be estimated: \\(\\sigma\\) and \\(k\\).\n\n\nCode\nfunction overdispersedModel(θ, Y::DataFrame, opts::Dict)\n\n    # Extract frequently used options\n    T = opts[\"T\"]\n    N = opts[\"N\"]\n    L = opts[\"L\"]\n    ω = opts[\"ω\"]\n\n    # Initialise output matrices\n    R = zeros(N, T)\n    I = zeros(N, T)\n    W = zeros(N, T)\n\n    # Sample from initial distributions\n    R[:,1] = rand.(opts[\"pR0\"], N)\n    I[:,1] = rand.(opts[\"pI0\"], N)\n\n    # Run the filter\n    for tt = 2:T\n\n        # Project according to the state-space model\n        R[:,tt] = exp.(rand.(Normal.(log.(R[:,tt-1]), θ[1])))\n        Λ = sum(I[:, (tt-1):-1:1] .* ω[1:(tt-1)]', dims=2)\n        I[:,tt] = rand.(Poisson.(R[:,tt] .* Λ))\n\n        # Weight according to the observation model\n        r = I[:,tt] / θ[2]\n        p = 1 / (1 + θ[2])\n        W[:,tt] = fastNegativeBinomialPDF(Y.Ct[tt], r, p)\n\n        # Resample\n        inds = wsample(1:N, W[:,tt], N; replace=true)\n        R[:, max(tt - L, 1):tt] = R[inds, max(tt - L, 1):tt]\n        I[:, max(tt - L, 1):tt] = I[inds, max(tt - L, 1):tt]\n\n    end\n\n    # Store output as three-dimensional array\n    X = zeros(N, T, 2)\n    X[:,:,1] = R\n    X[:,:,2] = I\n\n    return(X, W)\n\nend\n\n\nopts = Dict(\n\n    # Bootstrap filter options\n    \"T\" =&gt; 100, # Number of time-steps\n    \"N\" =&gt; 1000, # Number of particles\n    \"L\" =&gt; 50, # Fixed-lag resampling length\n    \"pR0\" =&gt; Uniform(0, 10), # Prior on Rt at t = 0\n    \"pI0\" =&gt; DiscreteUniform(1, 5), # Prior on It at t = 0\n    \"ω\" =&gt; pdf.(Gamma(2.36, 2.74), 1:100) / sum(pdf.(Gamma(2.36, 2.74), 1:100)), # Serial interval\n\n    # PMMH options\n    \"nChains\" =&gt; 3,\n    \"chunkSize\" =&gt; 100,\n    \"maxChunks\" =&gt; 50,\n    \"maxRhat\" =&gt; 1.05,\n    \"minESS\" =&gt; 100,\n    \"showChunkProgress\" =&gt; false,\n    \"propStdDevInit\" =&gt; [0.05, 0.1],\n    \"paramPriors\" =&gt; [Uniform(0, 1), Uniform(0, 10)],\n    \"initialParamSamplers\" =&gt; [Uniform(0.1, 0.3), Uniform(0.1, 0.5)],\n    \"paramNames\" =&gt; [\"σ\", \"k\"],\n\n    # Marginal posterior options\n    \"posteriorNumberOfParticles\" =&gt; 5000,\n    \"posteriorParamSamples\" =&gt; 100,\n    \"stateNames\" =&gt; [\"Rt\", \"It\"]\n\n)\n\n(df_states, df_params, θ, diag) = fitModel(overdispersedModel, Y, opts; verbose=false)\n\n\n\nθl = resizeParams(θ)\nplt = histogram(θl[:,2], bins=50, xlabel=\"k\", ylabel=\"Proportion of samples\", color=:darkorange, legend=false, normalize=:probability, size = (800,300), margins=3mm)\n\n\n\n\n\n\n\n\n\n\nCode\ndf_R = df_states[df_states.variable .== \"Rt\",:]\ndf_I = df_states[df_states.variable .== \"It\",:]\n\nplotR = plot(df_R.date, df_R.mean, ribbon=(df_R.mean-df_R.lower, df_R.upper-df_R.mean), label=false, color=:darkgreen, xlabel=\"Date\", ylabel=\"Reproduction number\", size=(800,300), margins=3mm)",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelling observation noise</span>"
    ]
  },
  {
    "objectID": "models_obsnoise.html#reporting-delays",
    "href": "models_obsnoise.html#reporting-delays",
    "title": "9  Modelling observation noise",
    "section": "9.4 Reporting delays",
    "text": "9.4 Reporting delays\nReporting delays are a common feature of epidemiological data, infections are vary rarely reported on the same day they occur. Accounting for these delays is important if we want our model to reflect the true underlying transmission dynamics. For this example, we follow Hendy et al. (2021) and assume a (discretised) gamma-distributed delay with a mean of 5.5 days and standard deviation of 2.3 days. The expected number of reported cases at time \\(t\\) is given by:\n\\[\n\\mu_t = \\sum_{u=1}^{u_{max}} I_{t-u} d_u\n\\]\nwhere \\(d_u = \\text{Gamma PDF}(u; \\mu=5.5, \\sigma=2.3)\\). Using the same negative binomial distribution as above, we can model the number of reported cases as: \\[\nC_t | \\mu_t, \\phi \\sim \\text{Negative Binomial}\\left(r = \\frac{\\mu_t}{k}, p=\\frac{1}{1 + k} \\right)\n\\]\n\nfunction delayedModel(θ, Y::DataFrame, opts::Dict)\n\n    # Extract frequently used options\n    T = opts[\"T\"]\n    N = opts[\"N\"]\n    L = opts[\"L\"]\n    ω = opts[\"ω\"]\n    delayDist = opts[\"delayDist\"]\n\n    # Initialise output matrices\n    R = zeros(N, T)\n    I = zeros(N, T)\n    W = zeros(N, T)\n\n    # Sample from initial distributions\n    R[:,1] = rand.(opts[\"pR0\"], N)\n    I[:,1] = rand.(opts[\"pI0\"], N)\n\n    # Run the filter\n    for tt = 2:T\n\n        # Project according to the state-space model\n        R[:,tt] = exp.(rand.(Normal.(log.(R[:,tt-1]), θ[1])))\n        Λ = sum(I[:, (tt-1):-1:1] .* ω[1:(tt-1)]', dims=2)\n        I[:,tt] = rand.(Poisson.(R[:,tt] .* Λ))\n\n        # Weight according to the observation model\n        μt = sum(I[:,(tt-1):-1:1] .* delayDist[1:(tt-1)]', dims=2) ./ sum(delayDist[1:(tt-1)])\n        r = vec(μt) ./ θ[2]\n        p = 1 / (1 + θ[2])\n        W[:,tt] = fastNegativeBinomialPDF(Y.Ct[tt], r, p)\n\n        # Resample\n        inds = wsample(1:N, W[:,tt], N; replace=true)\n        R[:, max(tt - L, 1):tt] = R[inds, max(tt - L, 1):tt]\n        I[:, max(tt - L, 1):tt] = I[inds, max(tt - L, 1):tt]\n\n    end\n\n    # Store output as three-dimensional array\n    X = zeros(N, T, 2)\n    X[:,:,1] = R\n    X[:,:,2] = I\n\n    return(X, W)\n\nend\n\nopts = Dict(\n\n    # Bootstrap filter options\n    \"T\" =&gt; size(Y, 1), # Number of time-steps\n    \"N\" =&gt; 2000, # Number of particles\n    \"L\" =&gt; 50, # Fixed-lag resampling length\n    \"ω\" =&gt; pdf.(Gamma(2.36, 2.74), 1:128), # Serial interval\n    \"delayDist\" =&gt; pdf.(Gamma(5.72, 0.96), 1:200), # Observation delay distribution\n    \"pR0\" =&gt; Uniform(0, 10), # Prior on Rt at t = 0\n    \"pI0\" =&gt; DiscreteUniform(1, 5), # Prior on I at t = 0\n\n    # PMMH options\n    \"nChains\" =&gt; 3, # Number of chains\n    \"chunkSize\" =&gt; 100, # Number of iterations\n    \"maxChunks\" =&gt; 50, # Maximum number of chunks\n    \"maxRhat\" =&gt; 1.05,  # Stopping criterion: maximum Rhat value\n    \"minESS\" =&gt; 100, # Stopping criterion: minimum effective sample size\n    \"showChunkProgress\" =&gt; true, # Whether to show progress of each chunk\n    \"propStdDevInit\" =&gt; [0.05, 0.1], # Initial proposal standard deviation (this is adaptively fit)\n    \"paramPriors\" =&gt; [Uniform(0, 1), Uniform(0, 10)],\n    \"initialParamSamplers\" =&gt; [Uniform(0.1, 0.3), Uniform(0.1, 0.5)],\n    \"paramLimits\" =&gt; [(0, 1), (0, 10)],\n    \"paramNames\" =&gt; [\"σ\", \"k\"],\n\n    # Marginal posterior options\n    \"posteriorNumberOfParticles\" =&gt; 10000,\n    \"posteriorParamSamples\" =&gt; 100,\n    \"stateNames\" =&gt; [\"Rt\", \"It\"]\n\n)\n\n(df_states, df_params, θ, diag) = fitModel(delayedModel, Y, opts; verbose=false, checkStdDevLogLik=false);\n\n\ndf_R_del = df_states[df_states.variable .== \"Rt\",:]\n\nplotR = plot(df_R.date, df_R.mean, ribbon=(df_R.mean-df_R.lower, df_R.upper-df_R.mean), color=:lightgreen, xlabel=\"Date\", ylabel=\"Reproduction number\", label=\"Without delay\", size=(800,400), margins=3mm)\nplotR = plot!(plotR, df_R_del.date, df_R_del.mean, ribbon=(df_R_del.mean-df_R_del.lower, df_R_del.upper-df_R_del.mean), color=:darkgreen, xlabel=\"Date\", ylabel=\"Reproduction number\", label=\"With delay\")\n\n\n\n\n\n\n\n\n\\(R_t\\) estimates that account for delayed reporting (dark green) are shifted earlier and feature more uncertainty than those that do not (light green). Accounting for these delays is important if we want our model to reflect the true underlying transmission dynamics, for example, if the model were to be used to evaluate the effectiveness of non-pharmaceutical interventions (NPIs) or to inform public health policy.",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelling observation noise</span>"
    ]
  },
  {
    "objectID": "models_obsnoise.html#day-of-the-week-effects",
    "href": "models_obsnoise.html#day-of-the-week-effects",
    "title": "9  Modelling observation noise",
    "section": "9.5 Day-of-the-week effects",
    "text": "9.5 Day-of-the-week effects\nFinally, day-of-the-week effects can have a large impact on epidemic models, if these effects are not appropriately treated as noise. Defining \\(c_i\\) as the relative reporting rate on day \\(i\\) (where \\(c_i &gt; 1\\) suggests a greater-than-average reporting rate), we update our model to allow for a day-of-the-week effect:\n\\[\nC_t | \\mu_t, \\phi, \\{c_i\\} \\sim \\text{Negative Binomial}\\left(r = \\frac{c_{\\text{mod}(t,7)}\\mu_t}{k}, p=\\frac{1}{1 + k} \\right)\n\\]\nwhere \\(c_i, i = 1, \\ldots, 7\\) are the relative reporting rates on each day of the week. Parameters \\(c_1\\) through \\(c_6\\) are estimated, with \\(c_7 = 7 - \\sum_{i=1}^6 c_i\\).\nWhen fitting this, we use data from the COVID-19 pandemic in Aotearoa New Zealand between 1 April 2024 and 9 July 2024, a 100-day period in which a clear day-of-the-week effect was observed.\n\nYin = loadData(\"NZCOVID_1APR2024\");\nY = Yin[1:100,:];\n\nfunction dayofweekModel(θ, Y::DataFrame, opts::Dict)\n\n    # Check that θ are valid\n    if sum(θ[3:8]) &gt; 7\n        error(\"Day of week multipliers must sum to less than or equal to 7\")\n    end\n\n    # Extract frequently used options\n    T = opts[\"T\"]\n    N = opts[\"N\"]\n    L = opts[\"L\"]\n    ω = opts[\"ω\"]\n    delayDist = opts[\"delayDist\"]\n\n    # Extract frequently used parameters\n    day_of_week_mult = vcat(θ[3:8], 7 - sum(θ[3:8]))\n\n    # Initialise output matrices\n    R = zeros(N, T)\n    I = zeros(N, T)\n    W = zeros(N, T)\n\n    # Sample from initial distributions\n    R[:,1] = rand(opts[\"pR0\"], N)\n    I[:,1] = rand(opts[\"pI0\"], N)\n\n    # Run the filter\n    for tt = 2:T\n\n        # Project according to the state-space model\n        R[:,tt] = exp.(rand.(Normal.(log.(R[:,tt-1]), θ[1])))\n        Λ = sum(I[:, (tt-1):-1:1] .* ω[1:(tt-1)]', dims=2) ./ sum(ω[1:(tt-1)])\n        I[:,tt] = rand.(Poisson.(R[:,tt] .* Λ))\n\n        # Weight according to the observation model\n        μt = day_of_week_mult[1 + tt % 7] * sum(I[:,(tt-1):-1:1] .* delayDist[1:(tt-1)]', dims=2) ./ sum(delayDist[1:(tt-1)])\n        r = vec(μt)/θ[2]\n        p = 1 / (1  + θ[2])\n        W[:,tt] = pdf.(NegativeBinomial.(r, p), Y.Ct[tt])\n\n        # Resample\n        inds = wsample(1:N, W[:,tt], N; replace=true)\n        R[:, max(tt - L, 1):tt] = R[inds, max(tt - L, 1):tt]\n        I[:, max(tt - L, 1):tt] = I[inds, max(tt - L, 1):tt]\n\n    end\n\n\n    # Store output as three-dimensional array\n    X = zeros(N, T, 2)\n    X[:,:,1] = R\n    X[:,:,2] = I\n\n    return(X, W)\n\nend\n\nopts = Dict(\n\n    # Bootstrap filter options\n    \"T\" =&gt; size(Y, 1), # Number of time-steps\n    \"N\" =&gt; 1000, # Number of particles\n    \"L\" =&gt; 50, # Fixed-lag resampling length\n    \"ω\" =&gt; pdf.(Gamma(2.36, 2.74), 1:128), # Serial interval\n    \"delayDist\" =&gt; pdf.(Gamma(5.72, 0.96), 1:200), # Observation delay distribution\n    \"pR0\" =&gt; Uniform(0, 10), # Prior on Rt at t = 0\n    \"pI0\" =&gt; DiscreteUniform(200, 600), # Prior on I at t = 0\n\n    # PMMH options\n    \"nChains\" =&gt; 3, # Number of chains\n    \"chunkSize\" =&gt; 100, # Number of iterations\n    \"maxChunks\" =&gt; 100, # Maximum number of chunks\n    \"maxRhat\" =&gt; 1.05,  # Stopping criterion: maximum Rhat value\n    \"minESS\" =&gt; 100, # Stopping criterion: minimum effective sample size\n    \"showChunkProgress\" =&gt; true, # Whether to show progress of each chunk\n    \"propStdDevInit\" =&gt; sqrt.([0.0002, 7e-6, 0.0005, 0.001, 0.001, 0.001, 0.001, 0.0005]), # Initial proposal standard deviation (this is adaptively fit)\n    \"paramPriors\" =&gt; vcat([Uniform(0, 1), Uniform(0, 10)], repeat([Uniform(0.1, 2)], 6)),\n    \"initialParamSamplers\" =&gt; [Uniform(0.07, 0.09), Uniform(4, 6), Uniform(0.75, 0.80), Uniform(1.3, 1.35), Uniform(1.2, 1.25), Uniform(1.1, 1.2), Uniform(0.95, 1.00), Uniform(0.9, 0.95)],\n    \"paramLimits\" =&gt; vcat([(0, 1), (0, 10)], repeat([(0.1, 2)], 6)),\n    \"paramNames\" =&gt; [\"sigma\", \"k\", \"Day1\", \"Day2\", \"Day3\", \"Day4\", \"Day5\", \"Day6\"],\n\n    # Marginal posterior options\n    \"posteriorNumberOfParticles\" =&gt; 10000,\n    \"posteriorParamSamples\" =&gt; 100,\n    \"stateNames\" =&gt; [\"Rt\", \"It\"]\n\n)\n\n(df_states, df_params, θ, diag) = fitModel(dayofweekModel, Y, opts; verbose=false, checkStdDevLogLik=false);\n\nθl = resizeParams(θ)\nC = zeros(size(θl, 1), 7)\nC[:,1:6] = θl[:,3:8]\nC[:,7] = 7 .- sum(C, dims=2)\nm = vec(mean(C, dims=1))\nl = [quantile(C[:,i], 0.025) for i in 1:7]\nu = [quantile(C[:,i], 0.975) for i in 1:7]\n\ndf_R = df_states[df_states.variable .== \"Rt\",:]\n\nday_of_week_label = [\"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thur\", \"Fri\", \"Sat\"]\n\nplt_cases = bar(Y.date, Y.Ct, color=:darkblue, size=(800,400), xlabel=\"Date\", ylabel=\"Reported cases\", label=false, margins=3mm)\nplt_Rt = plot(df_R.date, df_R.mean, ribbon=(df_R.mean-df_R.lower, df_R.upper-df_R.mean), color=:darkgreen, xlabel=\"Date\", ylabel=\"Reproduction number\", label=false)\n\nplt_dow = scatter(day_of_week_label, m, errorbar=(m-l, u-m),\n    xlabel=\"Day of week\", ylabel=\"Relative reporting rate\", legend=false,\n    linewidth=3, color=:darkred, lc=:darkred, markercolor=:darkred, markersize=5,\n    yguidefont=font(10), xguidefont=font(10), size=(400,300))\n\nplt = plot(plt_cases, plt_Rt, plt_dow, layout=(3,1), size=(800,600), legend=false)\n\n\n\n\n\n\n\n\nDemonstrating a clear day-of-the-week effect, with reporting more likely to occur on weekdays than weekends.",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelling observation noise</span>"
    ]
  },
  {
    "objectID": "models_obsnoise.html#supplementary-material",
    "href": "models_obsnoise.html#supplementary-material",
    "title": "9  Modelling observation noise",
    "section": "9.6 Supplementary material",
    "text": "9.6 Supplementary material\n\n9.6.1 “Estimating” the underreporting rate\nWe state in Section 9.1 that the reporting rate \\(\\rho\\) is not identifiable from these data. Actually, as \\(\\rho\\) determines the observation variance, the data likely contain some information about this term.\nThe observation variance of \\(C_t\\) is given by:\n\\[\nVar(C_t) = I_t \\rho (1 - \\rho)\n\\]\nHolding \\(E[C_t] = I_t \\rho\\) fixed, we can see that the observation variance is a decreasing function of \\(\\rho\\). That is, the smaller the estimated value of \\(\\rho\\), the greater the estimated observation variance. This is somewhat approximate, as the model must choose a temporally fixed value of \\(\\rho\\), but is free to vary \\(I_t\\) (within the constraints of the epidemc model).\nA combination of model misspecification (reported cases \\(C_t\\) are unlikely to perfectly follow a binomial distribution with mean \\(I_t \\rho\\)) and limited data (even if the model was correctly specified, a large quantity of data would be required to estimate this parameter) limit the real-world applicability of this approach for inference. In terms of predictions, where identifiabilitiy is not required, this may be a useful approach.\n\n\nCode\nY = loadData(\"NZCOVID\")\n\nfunction underreportingModel2(θ, Y::DataFrame, opts::Dict)\n\n    # Extract frequently used options\n    T = opts[\"T\"]\n    N = opts[\"N\"]\n    L = opts[\"L\"]\n    ω = opts[\"ω\"]\n\n    # Initialise output matrices\n    R = zeros(N, T)\n    I = zeros(N, T)\n    W = zeros(N, T)\n\n    # Sample from initial distributions\n    R[:,1] = rand.(opts[\"pR0\"], N)\n    I[:,1] = rand.(opts[\"pI0\"], N)\n\n    # Run the filter\n    for tt = 2:T\n\n        # Project according to the state-space model\n        R[:,tt] = exp.(rand.(Normal.(log.(R[:,tt-1]), θ[1])))\n        Λ = sum(I[:, (tt-1):-1:1] .* ω[1:(tt-1)]', dims=2)\n        I[:,tt] = rand.(Poisson.(R[:,tt] .* Λ))\n\n        # Weight according to the observation model\n        W[:,tt] = pdf.(Binomial.(I[:,tt], θ[2]), Y.Ct[tt])\n\n        # Resample\n        inds = wsample(1:N, W[:,tt], N; replace=true)\n        R[:, max(tt - L, 1):tt] = R[inds, max(tt - L, 1):tt]\n        I[:, max(tt - L, 1):tt] = I[inds, max(tt - L, 1):tt]\n\n    end\n\n    # Store output as three-dimensional array\n    X = zeros(N, T, 2)\n    X[:,:,1] = R\n    X[:,:,2] = I\n\n    return(X, W)\n\nend\n\nopts = Dict(\n\n    # Bootstrap filter options\n    \"T\" =&gt; 100, # Number of time-steps\n    \"N\" =&gt; 1000, # Number of particles\n    \"L\" =&gt; 50, # Fixed-lag resampling length\n    \"pR0\" =&gt; Uniform(0, 10), # Prior on Rt at t = 0\n    \"pI0\" =&gt; DiscreteUniform(1, 5), # Prior on It at t = 0\n    \"ω\" =&gt; pdf.(Gamma(2.36, 2.74), 1:100) / sum(pdf.(Gamma(2.36, 2.74), 1:100)), # Serial interval\n\n    # PMMH options\n    \"nChains\" =&gt; 3,\n    \"chunkSize\" =&gt; 100,\n    \"maxChunks\" =&gt; 50,\n    \"maxRhat\" =&gt; 1.05,\n    \"minESS\" =&gt; 500,\n    \"showChunkProgress\" =&gt; false,\n    \"propStdDevInit\" =&gt; [0.1, 0.1],\n    \"paramPriors\" =&gt; [Uniform(0, 1), Uniform(0, 1)],\n    \"initialParamSamplers\" =&gt; [Uniform(0.1, 0.3), Uniform(0.1, 0.9)],\n    \"paramNames\" =&gt; [\"σ\", \"ρ\"],\n\n    # Marginal posterior options\n    \"posteriorNumberOfParticles\" =&gt; 10000,\n    \"posteriorParamSamples\" =&gt; 100,\n    \"stateNames\" =&gt; [\"Rt\", \"It\"]\n);\n\n(df_states, df_params, θ, diag) = fitModel(underreportingModel2, Y, opts)\n\n#| code-fold: true\n#| \ndf_R = df_states[df_states.variable .== \"Rt\",:]\ndf_I = df_states[df_states.variable .== \"It\",:]\n\nplotR = plot(df_R.date, df_R.mean, ribbon=(df_R.mean-df_R.lower, df_R.upper-df_R.mean), label=false, color=:darkgreen, xlabel=\"Date\", ylabel=\"Reproduction number\", size=(800,300), margins=3mm)\n\n\ndisplay(df_params)\n\n\n2 rows × 6 columnsparammlurhatessStringFloat64Float64Float64Float64Float641σ0.2222930.1518750.325691.00763517.5012ρ0.3057480.03871350.8645691.00584527.193\n\n\nThe model has estimated \\(\\rho = 0.31 \\ (0.04, 0.86)\\). Plotting the samples of \\(\\rho\\) below demonstrates a clear non-uniform posterior distribution, so there is some information about this parameter in the data, even if we are not necessarily estimating an interpretable quantity.\n\nθl = resizeParams(θ)\nplt = histogram(θl[:,2], bins=50, xlabel=\"ρ\", ylabel=\"Proportion of samples\", color=:darkorange, legend=false, normalize=:probability, size = (600,400), margins=3mm)\n\n\n\n\n\n\n\n\n\n\n\n\nHendy, Shaun, Nicholas Steyn, Alex James, Michael J. Plank, Kate Hannah, Rachelle N. Binny, and Audrey Lustig. 2021. “Mathematical Modelling to Inform New Zealand’s COVID-19 Response.” Journal of the Royal Society of New Zealand 51 (sup1): S86–106. https://doi.org/10.1080/03036758.2021.1876111.",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelling observation noise</span>"
    ]
  },
  {
    "objectID": "models_obsnoise.html#footnotes",
    "href": "models_obsnoise.html#footnotes",
    "title": "9  Modelling observation noise",
    "section": "",
    "text": "The beta-binomial explicitly assumes that \\(I_t\\) are binomially distributed with random probability \\(\\rho \\sim Beta(\\alpha_t, \\beta_t)\\). Under this interpretation, \\((1/\\phi - 1)\\) can be thought of as the number of “prior trials”, and \\(\\alpha_t\\) the number of “prior successes”.↩︎",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelling observation noise</span>"
    ]
  },
  {
    "objectID": "models_tempagg.html",
    "href": "models_tempagg.html",
    "title": "10  Temporally aggregated data",
    "section": "",
    "text": "10.1 Modelling temporally aggregated data\nTemporally aggregated incidence data (such as weekly instead of daily reported cases) can make \\(R_t\\) estimation using tools like EpiEstim and EpiFilter difficult, particularly when the duration of the aggregation period exceeds the scale at which transmission occurs. The fundamental issue is methods like EpiEstim and EpiFilter assume that reported cases at the current time-step are caused only by reported cases on previous time-steps, whereas in aggregated data, we must also account for within-period transmission.\nExisting approaches to estimating \\(R_t\\) from temporally aggregated data all reconstruct finer-grained incidence data from the aggregated data, before applying more standard methods. For example, Nash et al. (2023) use an Expectation-Maximisation algorithm to infer daily incidence and then apply an EpiEstim-type method, while I. Ogi-Gittins et al. (2024) pair an exact form of approximate Bayesian computation with a simulation based approach to infer sub-daily incidence. One method that does not require substantial adaption is EpiNow2 Abbott et al. (2020), which models the latent infections already.\nGiven an appropriate inference method, there are also cases in which it makes sense to model temporally aggregated data. If day-of-the-week effects are present, large, and inconsistent, then modelling weekly-aggregated data can decrease observation variance. From a practical modelling perspective, in the SMC context, aggregated data feature fewer observations, thus requiring fewer particles to estimate the model log-likelihood with the same precision.\nWe can use any model which separates latent infections \\(I_t\\) from reported cases \\(C_t\\), for example, the model introduced in ?sec-models-obsnoise-obsnoisenounder (where subscript \\(t\\) still denotes time in days):\n\\[\n\\begin{align}\n\\log R_t | \\log R_{t-1} &\\sim \\text{Normal}(\\log R_{t-1}, \\sigma) \\\\\nI_t | R_t, I_{1:t-1} &\\sim \\text{Poisson}(R_t \\Lambda_t)\n\\end{align}\n\\tag{10.1}\\]\nwhere the observation model is modified slightly to reflect the assumed aggregation (using weekly as an example):\n\\[\nC_t | I_{1:t} \\sim \\text{Negative binomial}\\left(r = \\frac{\\sum_{s=0}^6 I_{t-s}}{k}, p = \\frac{1}{1+k} \\right), \\ \\ \\text{if} \\ \\text{mod}(t, 7) = 0\n\\]\nAlgorithmically, we just skip the weighting and resampling steps on the time steps for which we have no observations.",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Temporally aggregated data</span>"
    ]
  },
  {
    "objectID": "models_tempagg.html#modelling-temporally-aggregated-data",
    "href": "models_tempagg.html#modelling-temporally-aggregated-data",
    "title": "10  Temporally aggregated data",
    "section": "",
    "text": "10.1.1 Example: Influenza in Wales\nBorrowing the example data and generation time distribution from I. Ogi-Gittins et al. (2024), we estimate \\(R_t\\) in Wales over two influenza seaons (2019-2020 and 2022-2023). The data are reported as weekly case counts, which as above, we assume reflect all reported cases over the preceding week. We edit the bootstrap filter to only weight and resample on days on which data are observed:\nfor tt = 2:T\n        \n    # Project according to the state-space model\n    I[:,tt] = ...\n        \n    # Weight according to the observation model,\n    # ... but only on the days that we observe data\n    if tt % 7 == 0\n            \n        # Fetch expected reported cases (sum over the preceding 7 days)\n        μt = sum(I[:,(tt-6):1:tt], dims=2)\n            \n        # Calculate weights\n        W[:,tt] = ... # Some function of expected reported cases\n            \n        # Resample\n        ...\n        \n    end\n        \nend\nFitting the model and plotting the estiamtes of \\(R_t\\) and daily incidence \\(I_t\\) produces:\n\n\nCode\nusing Plots, Measures\n\ninclude(\"../src/LoadData.jl\")\ninclude(\"../src/PMMH.jl\")\ninclude(\"../src/FitModel.jl\")\ninclude(\"../src/MarginalPosterior.jl\")\ninclude(\"../src/Support.jl\")\n\nfunction temporallyAggregatedModel(θ, Y::DataFrame, opts::Dict)\n    \n    # Extract frequently used options\n    T = opts[\"T\"]\n    N = opts[\"N\"]\n    L = opts[\"L\"]\n    ω = opts[\"ω\"]\n    \n    # Initialise output matrices\n    R = zeros(N, T)\n    I = zeros(N, T)\n    W = ones(N, T)\n        \n    # Sample from initial distributions\n    R[:,1] = rand(opts[\"pR0\"], N)\n    I[:,1] = rand(opts[\"pI0\"], N)\n    \n    # Run the filter\n    for tt = 2:T\n        \n        # Project according to the state-space model\n        R[:,tt] = exp.(rand.(Normal.(log.(R[:,tt-1]), θ[1])))\n        Λ = sum(I[:, (tt-1):-1:1] .* ω[1:(tt-1)]', dims=2) ./ sum(ω[1:(tt-1)])\n        I[:,tt] = rand.(Poisson.(R[:,tt] .* Λ))\n        \n        # Weight according to the observation model, but only on the day that we observe data\n        if tt % 7 == 0\n            \n            # Fetch expected reported cases\n            μt = sum(I[:,(tt-6):1:tt], dims=2)\n            \n            # Calculate weights\n            r = max.(μt, 1)/θ[2]\n            p = 1 / (1 + θ[2])\n            W[:,tt] = pdf.(NegativeBinomial.(r, p), Y.Ct[tt])\n            \n            # Resample\n            inds = wsample(1:N, W[:,tt], N; replace=true)\n            R[:, max(tt - L, 1):tt] = R[inds, max(tt - L, 1):tt]\n            I[:, max(tt - L, 1):tt] = I[inds, max(tt - L, 1):tt]\n            \n        end\n        \n    end\n    \n    # Store output as three-dimensional array\n    X = zeros(N, T, 2)\n    X[:,:,1] = R\n    X[:,:,2] = I\n    \n    # Forecast\n    return(X, W)\n    \nend\n\n\n# Specify generation time distribution parameters\ngamma_mean = 2.6\ngamma_sd = 1.3\ngamma_shape = gamma_mean^2 / gamma_sd^2\ngamma_scale = gamma_sd^2 / gamma_mean\n\nopts = Dict(\n\n    # Bootstrap filter options\n    \"T\" =&gt; missing, # Number of time-steps\n    \"N\" =&gt; 1000, # Number of particles\n    \"L\" =&gt; 50, # Fixed-lag resampling length\n    \"pR0\" =&gt; Uniform(0.5, 3), # Prior on Rt at t = 0\n    \"pI0\" =&gt; DiscreteUniform(1, 5), # Prior on It at t = 0\n\n    # Generation time distribution\n    \"ω\" =&gt; pdf.(Gamma(gamma_shape, gamma_scale), 1:100) / sum(pdf.(Gamma(gamma_shape, gamma_scale), 1:100)), # Serial interval: Gamma with mean 2.6 and sd 1.3 days\n\n    # PMMH options\n    \"nChains\" =&gt; 3,\n    \"chunkSize\" =&gt; 100,\n    \"maxChunks\" =&gt; 50,\n    \"maxRhat\" =&gt; 1.05,\n    \"minESS\" =&gt; 100,\n    \"showChunkProgress\" =&gt; false,\n    \"propStdDevInit\" =&gt; [0.05, 0.5],\n    \"paramPriors\" =&gt; [Uniform(0, 1), Uniform(0, 10)],\n    \"initialParamSamplers\" =&gt; [Uniform(0.1, 0.3), Uniform(4, 6)],\n    \"paramNames\" =&gt; [\"σ\", \"k\"],\n\n    # Marginal posterior options\n    \"posteriorNumberOfParticles\" =&gt; 5000,\n    \"posteriorParamSamples\" =&gt; 100,\n    \"stateNames\" =&gt; [\"Rt\", \"It\"]\n\n)\n\n\n# Load data\nY1 = loadData(\"Ogi-Gittins-201920\");\nopts[\"T\"] = size(Y1)[1]\nopts[\"pI0\"] = DiscreteUniform(5, 10)\n\n# Fit the model\n(df_states1, df_params1, θ1, diag1) = fitModel(temporallyAggregatedModel, Y1, opts; verbose=false)\ndf_states1.Date = [Y1.Date; Y1.Date]\n\n# Fit the second set of data\nY2 = loadData(\"Ogi-Gittins-202223\");\nopts[\"T\"] = size(Y2)[1]\nopts[\"pI0\"] = DiscreteUniform(20, 40)\n\n# Fit the model to the second set of data\n(df_states2, df_params2, θ2, diag2) = fitModel(temporallyAggregatedModel, Y2, opts; verbose=false)\ndf_states2.Date = [Y2.Date; Y2.Date]\n\n# Plot the data and the results\ndf_R_1 = df_states1[df_states1.variable .== \"Rt\", :]\ndf_I_1 = df_states1[df_states1.variable .== \"It\", :]\ndf_R_2 = df_states2[df_states2.variable .== \"Rt\", :]\ndf_I_2 = df_states2[df_states2.variable .== \"It\", :]\n\nplot_R_1 = plot(df_R_1.Date, df_R_1.mean, ribbon=(df_R_1.mean .- df_R_1.lower, df_R_1.upper .- df_R_1.mean), xlabel=\"Date\", ylabel=\"Reproduction number\", legend=false)\nplot_I_1 = plot(df_I_1.Date, df_I_1.mean, ribbon=(df_I_1.mean .- df_I_1.lower, df_I_1.upper .- df_I_1.mean), xlabel=\"Date\", ylabel=\"Infection incidence\", legend=false)\nplot_R_2 = plot(df_R_2.Date, df_R_2.mean, ribbon=(df_R_2.mean .- df_R_2.lower, df_R_2.upper .- df_R_2.mean), xlabel=\"Date\", ylabel=\"Reproduction number\", legend=false)\nplot_I_2 = plot(df_I_2.Date, df_I_2.mean, ribbon=(df_I_2.mean .- df_I_2.lower, df_I_2.upper .- df_I_2.mean), xlabel=\"Date\", ylabel=\"Infection incidence\", legend=false)\nplot_C_1 = bar(Y1.Date, Y1.Ct, xlabel=\"Date\", ylabel=\"Weekly reported cases\", legend=false, bar_width=3)\nplot_C_2 = bar(Y2.Date, Y2.Ct, xlabel=\"Date\", ylabel=\"Weekly reported cases\", legend=false, bar_width=3)\n\nplot_all_1 = plot(plot_R_1, plot_I_1, plot_C_1, layout=(3, 1), size=(800, 800))\nplot_all_2 = plot(plot_R_2, plot_I_2, plot_C_2, layout=(3, 1), size=(800, 800))\np = plot(plot_all_1, plot_all_2, layout=(1, 2), size=(1600, 800), titlefontsize=16, margins=6mm)\np = plot!(p[1], title=\"Influenza in Wales, 2019-20\")\np = plot!(p[4], title=\"Influenza in Wales, 2022-23\")\ndisplay(p)",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Temporally aggregated data</span>"
    ]
  },
  {
    "objectID": "models_tempagg.html#concluding-remarks",
    "href": "models_tempagg.html#concluding-remarks",
    "title": "10  Temporally aggregated data",
    "section": "10.2 Concluding remarks",
    "text": "10.2 Concluding remarks\nThe approach highlighted above can be viewed as a generalisation of the two Ogi-Gittins methods for modelling temporally aggregated data. If \\(R_t\\) is assumed to be weekly-constant and independent, and there is assumed to be no observation noise (i.e. the observation distribution for total infection incidence is dirac at the number of observed cases), then the model of (I. Ogi-Gittins et al. 2024) is recovered. Alternatively, if binomial underreporting is assumed, then the model of (Isaac Ogi-Gittins et al. 2025) is recovered.\nWe also highlight the flexibility of this method: the observation model can be modified in a variety of ways. The weekly reporting window is arbitrary, and even irregular reporting windows can be implemented. Furthermore, the daily time steps are also arbitrary, and a finer discretisation could be used (for infectious diseases with very short serial intervals) or a coarser discretiation could be used (for infectious diseases with long serial intervals, to save computation time).\n\n\n\n\nAbbott, Sam, Joel Hellewell, Robin N. Thompson, Katharine Sherratt, Hamish P. Gibbs, Nikos I. Bosse, James D. Munday, et al. 2020. “Estimating the Time-Varying Reproduction Number of SARS-CoV-2 Using National and Subnational Case Counts.” Wellcome Open Research 5 (December): 112. https://doi.org/10.12688/wellcomeopenres.16006.2.\n\n\nNash, Rebecca K., Samir Bhatt, Anne Cori, and Pierre Nouvellet. 2023. “Estimating the Epidemic Reproduction Number from Temporally Aggregated Incidence Data: A Statistical Modelling Approach and Software Tool.” Edited by Eric Hy Lau. PLOS Computational Biology 19 (8): e1011439. https://doi.org/10.1371/journal.pcbi.1011439.\n\n\nOgi-Gittins, I., W. S. Hart, J. Song, R. K. Nash, J. Polonsky, A. Cori, E. M. Hill, and R. N. Thompson. 2024. “A Simulation-Based Approach for Estimating the Time-Dependent Reproduction Number from Temporally Aggregated Disease Incidence Time Series Data.” Epidemics 47: 100773. https://doi.org/10.1016/j.epidem.2024.100773.\n\n\nOgi-Gittins, Isaac, Nicholas Steyn, Jonathan Polonsky, William S. Hart, Mory Keita, Steve Ahuka-Mundeke, Edward M. Hill, and Robin N. Thompson. 2025. “Simulation-Based Inference of the Time-Dependent Reproduction Number from Temporally Aggregated and Under-Reported Disease Incidence Time Series Data.” Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 383 (2293): 20240412. https://doi.org/10.1098/rsta.2024.0412.",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Temporally aggregated data</span>"
    ]
  },
  {
    "objectID": "models_imports.html",
    "href": "models_imports.html",
    "title": "11  Imported cases",
    "section": "",
    "text": "11.1 Without quarantine\nWe suggested in section ?sec-smc-data that separating imported from local cases may be critical when modelling the example data (the first 100 days of the COVID-19 pandemic in New Zealand). Separating the graph of reported cases into local and imported cases demonstrates why:\nThe problem of modelling imported cases has previously been covered by Thompson et al. (2019), …, and …\nThis chapter demonstrates one way in which local and imported cases can be distinguished in the sequential hidden-state framework.\nWe retain the hidden-state model from Chapter 8:\n\\[\n\\log R_t \\sim \\text{Normal}(\\log R_{t-1}, \\sigma)\n\\tag{11.1}\\]\nand now assume that only local cases \\(L_t\\) are infected by past local and imported \\(M_t\\) cases:\n\\[\nL_t | R_t \\sim \\text{Poisson}\\left(R_t \\Lambda_t^{(m)}\\right)\n\\tag{11.2}\\]\nwhere\n\\[\n\\Lambda_t^{(m)} = \\sum_{u=1}^{u_{max}} \\omega_u \\left(L_{t-u} + M_{t-u}\\right)\n\\tag{11.3}\\]\nThe bootstrap filter for this model is nearly identical to the simple model, we simply change one line (#TODO: install highlight extension and highlight):\nCode\nfunction importedModel(σ, Y::DataFrame, opts::Dict)\n\n    # Extract frequently used options\n    T = opts[\"T\"]\n    N = opts[\"N\"]\n    L = opts[\"L\"]\n\n    # Initialise output matrices\n    R = zeros(N, T) # Using R instead of X to highlight we're estimating Rt\n    W = zeros(N, T)\n\n    # Sample from initial distribution\n    R[:,1] = rand.(opts[\"pR0\"], N)\n\n    # Run the filter\n    for tt = 2:T\n\n        # Project according to the state-space model\n        R[:,tt] = exp.(rand.(Normal.(log.(R[:,tt-1]), σ)))\n\n        # Weight according to the observation model\n        Λ = sum(Y.Ct[tt-1:-1:1] .* ω[1:tt-1])\n        W[:,tt] = pdf.(Poisson.(R[:,tt] .* Λ), Y.local[tt]) # &lt;- This line is the only line that has changed!\n\n        # Resample\n        inds = wsample(1:N, W[:,tt], N; replace=true)\n        R[:, max(tt - L, 1):tt] = R[inds, max(tt - L, 1):tt]\n\n    end\n\n    return(R, W)\n\nend\nFitting the model and plotting \\(R_t\\) against our original estmiates reveals substantial differences:\n[Single figure to go here]",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Imported cases</span>"
    ]
  },
  {
    "objectID": "models_imports.html#with-quarantine",
    "href": "models_imports.html#with-quarantine",
    "title": "11  Imported cases",
    "section": "11.2 With quarantine",
    "text": "11.2 With quarantine\nThe model above assumed that imported cases are just as infectious as local cases. In",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Imported cases</span>"
    ]
  },
  {
    "objectID": "models_imports.html#concluding-remarks",
    "href": "models_imports.html#concluding-remarks",
    "title": "11  Imported cases",
    "section": "11.3 Concluding remarks",
    "text": "11.3 Concluding remarks\n\n\n\n\nThompson, R. N., J. E. Stockwin, R. D. van Gaalen, J. A. Polonsky, Z. N. Kamvar, P. A. Demarsh, E. Dahlqwist, et al. 2019. “Improved Inference of Time-Varying Reproduction Numbers During Infectious Disease Outbreaks.” Epidemics 29 (December): 100356. https://doi.org/10.1016/j.epidem.2019.100356.",
    "crumbs": [
      "Models",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Imported cases</span>"
    ]
  },
  {
    "objectID": "eval-additionalnotes.html",
    "href": "eval-additionalnotes.html",
    "title": "14  Additional notes on model evaluation",
    "section": "",
    "text": "14.1 KL-divergence of nested models\nThe model evaluation section remains under-developed. Included here are notes that will hopefully eventually find a more suitable home.\n[#TODO]",
    "crumbs": [
      "Evaluation",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Additional notes on model evaluation</span>"
    ]
  },
  {
    "objectID": "othermethods_epiestim.html",
    "href": "othermethods_epiestim.html",
    "title": "15  EpiEstim",
    "section": "",
    "text": "15.1 Model definition\nWe present Julia implementations of EpiEstim (this chapter), EpiFilter (Chapter 16), and a heavily simplified version of (Chapter 17). The (approximate) EpiNow2 implementation also requires the installation of Stan, while EpiEstim and EpiFilter are implemented natively.\nMuch of the code for EpiEstim and EpiFilter has been reproduced from previous work [CITE PAPER WITH KRIS].\nEpiEstim assumes that (typically daily) \\(R_t\\) is fixed over trailing windows of length \\(k\\). A common choice of this parameter value is \\(k = 7\\), in which case estimates produced by EpiEstim are referred to as weekly estimates. The Poisson renewal model is used, giving likelihood:\n\\[\nL(R_t | C_{1:t}, k) = \\prod_{s = t - k + 1}^t \\text{Poisson PDF}\\left(C_s; R_t \\Lambda_s^c \\right)\n\\]\nwhere \\(\\Lambda_t^c\\) is the force-of-infection of reported cases. A \\(\\text{Gamma}(\\alpha_0, \\beta_0)\\) prior distribution is placed on \\(R_t\\), such that the posterior distribution can be shown to be:\n\\[\nP(R_t | C_{1:t}, k) \\sim \\text{Gamma}\\left(\\alpha = \\alpha_0 + \\sum_{s=t-k+1}^t C_s, \\beta = \\beta_0 + \\sum_{s=t-k+1}^t \\Lambda_s \\right)\n\\]",
    "crumbs": [
      "Other methods",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>EpiEstim</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Abbott, Sam, Joel Hellewell, Robin N. Thompson, Katharine Sherratt,\nHamish P. Gibbs, Nikos I. Bosse, James D. Munday, et al. 2020.\n“Estimating the Time-Varying Reproduction Number of\nSARS-CoV-2 Using National and Subnational Case\nCounts.” Wellcome Open Research 5 (December): 112. https://doi.org/10.12688/wellcomeopenres.16006.2.\n\n\nAzmon, Amin, Christel Faes, and Niel Hens. 2014. “On the\nEstimation of the Reproduction Number Based on Misreported Epidemic\nData.” Statistics in Medicine 33 (7): 1176–92. https://doi.org/10.1002/sim.6015.\n\n\nBanholzer, Nicolas, Thomas Mellan, H. Juliette T. Unwin, Stefan\nFeuerriegel, Swapnil Mishra, and Samir Bhatt. 2023. “A Comparison\nof Short-Term Probabilistic Forecasts for the Incidence of\nCOVID-19 Using Mechanistic and Statistical Time Series\nModels.” arXiv. https://doi.org/10.48550/arXiv.2305.00933.\n\n\nCori, Anne, Neil M. Ferguson, Christophe Fraser, and Simon Cauchemez.\n2013. “A New Framework and Software to\nEstimate Time-Varying Reproduction Numbers During\nEpidemics.” American Journal of Epidemiology 178\n(9): 1505–12. https://doi.org/10.1093/aje/kwt133.\n\n\nCori, Anne, and Adam Kucharski. 2024. “Inference of Epidemic\nDynamics in the COVID-19 Era and Beyond.”\nEpidemics 48 (September): 100784. https://doi.org/10.1016/j.epidem.2024.100784.\n\n\nCreal, Drew. 2012. “A Survey of Sequential\nMonte Carlo Methods for Economics and\nFinance.” Econometric Reviews 31 (3):\n245–96. https://doi.org/10.1080/07474938.2011.607333.\n\n\nCreswell, Richard, Martin Robinson, David Gavaghan, Kris V. Parag, Chon\nLok Lei, and Ben Lambert. 2023. “A Bayesian\nNonparametric Method for Detecting Rapid Changes in Disease\nTransmission.” Journal of Theoretical Biology 558\n(February): 111351. https://doi.org/10.1016/j.jtbi.2022.111351.\n\n\nDoucet, Arnaud, and Adam M. Johansen. 2011. “A Tutorial on\nParticle Filtering and Smoothing : Fiteen Years Later.” In,\nedited by Dan Crisan and Boris Rozovskii, 656–705. Oxford ; N.Y.: Oxford\nUniversity Press.\n\n\nElvira, Víctor, Luca Martino, and Christian P. Robert. 2022.\n“Rethinking the Effective Sample Size.”\nInternational Statistical Review 90 (3): 525–50. https://doi.org/10.1111/insr.12500.\n\n\nEndo, Akira, Edwin van Leeuwen, and Marc Baguelin. 2019.\n“Introduction to Particle Markov-chain Monte\nCarlo for Disease Dynamics Modellers.” Epidemics\n29 (December): 100363. https://doi.org/10.1016/j.epidem.2019.100363.\n\n\nEvensen, Geir, Femke C. Vossepoel, and Peter Jan Van Leeuwen. 2022.\nData Assimilation Fundamentals: A Unified\nFormulation of the State and Parameter\nEstimation Problem. Springer Textbooks in\nEarth Sciences, Geography and\nEnvironment. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-030-96709-3.\n\n\nFerguson, N, D Laydon, G Nedjati Gilani, N Imai, K Ainslie, M Baguelin,\nS Bhatia, et al. 2020. “Report 9: Impact of\nNon-Pharmaceutical Interventions (NPIs) to Reduce\nCOVID19 Mortality and Healthcare Demand.” Imperial\nCollege London. https://doi.org/10.25561/77482.\n\n\nFlaxman, Seth, Swapnil Mishra, Axel Gandy, H. Juliette T. Unwin, Thomas\nA. Mellan, Helen Coupland, Charles Whittaker, et al. 2020.\n“Estimating the Effects of Non-Pharmaceutical Interventions on\nCOVID-19 in Europe.” Nature\n584 (7820): 257–61. https://doi.org/10.1038/s41586-020-2405-7.\n\n\nHendy, Shaun, Nicholas Steyn, Alex James, Michael J. Plank, Kate Hannah,\nRachelle N. Binny, and Audrey Lustig. 2021. “Mathematical\nModelling to Inform New Zealand’s COVID-19\nResponse.” Journal of the Royal Society of New Zealand\n51 (sup1): S86–106. https://doi.org/10.1080/03036758.2021.1876111.\n\n\nIonides, E. L., C. Bretó, and A. A. King. 2006. “Inference for\nNonlinear Dynamical Systems.” Proceedings of the National\nAcademy of Sciences 103 (49): 18438–43. https://doi.org/10.1073/pnas.0603181103.\n\n\nIonides, Edward L., Anindya Bhadra, Yves Atchadé, and Aaron King. 2011.\n“Iterated Filtering.” The Annals of Statistics 39\n(3). https://doi.org/10.1214/11-AOS886.\n\n\nIonides, Edward L., Dao Nguyen, Yves Atchadé, Stilian Stoev, and Aaron\nA. King. 2015. “Inference for Dynamic and Latent Variable Models\nvia Iterated, Perturbed Bayes Maps.” Proceedings\nof the National Academy of Sciences 112 (3): 719–24. https://doi.org/10.1073/pnas.1410597112.\n\n\nKantas, Nikolas, Arnaud Doucet, Sumeetpal S. Singh, Jan Maciejowski, and\nNicolas Chopin. 2015. “On Particle Methods for\nParameter Estimation in State-Space\nModels.” Statistical Science 30 (3): 328–51. https://doi.org/10.1214/14-STS511.\n\n\nKing, Aaron A., Dao Nguyen, and Edward L. Ionides. 2016.\n“Statistical Inference for Partially Observed\nMarkov Processes via the R\nPackage Pomp.”\nJournal of Statistical Software 69 (12). https://doi.org/10.18637/jss.v069.i12.\n\n\nMinistry of Health NZ. 2024. “New Zealand COVID-19\nData.”\n\n\nNash, Rebecca K., Samir Bhatt, Anne Cori, and Pierre Nouvellet. 2023.\n“Estimating the Epidemic Reproduction Number from Temporally\nAggregated Incidence Data: A Statistical Modelling Approach\nand Software Tool.” Edited by Eric Hy Lau. PLOS Computational\nBiology 19 (8): e1011439. https://doi.org/10.1371/journal.pcbi.1011439.\n\n\nOgi-Gittins, I., W. S. Hart, J. Song, R. K. Nash, J. Polonsky, A. Cori,\nE. M. Hill, and R. N. Thompson. 2024. “A Simulation-Based Approach\nfor Estimating the Time-Dependent Reproduction Number from Temporally\nAggregated Disease Incidence Time Series Data.”\nEpidemics 47: 100773. https://doi.org/10.1016/j.epidem.2024.100773.\n\n\nOgi-Gittins, Isaac, Nicholas Steyn, Jonathan Polonsky, William S. Hart,\nMory Keita, Steve Ahuka-Mundeke, Edward M. Hill, and Robin N. Thompson.\n2025. “Simulation-Based Inference of the Time-Dependent\nReproduction Number from Temporally Aggregated and Under-Reported\nDisease Incidence Time Series Data.” Philosophical\nTransactions of the Royal Society A: Mathematical, Physical and\nEngineering Sciences 383 (2293): 20240412. https://doi.org/10.1098/rsta.2024.0412.\n\n\nParag, Kris V. 2021. “Improved Estimation of Time-Varying\nReproduction Numbers at Low Case Incidence and Between Epidemic\nWaves.” PLOS Computational Biology 17 (9): e1009347. https://doi.org/10.1371/journal.pcbi.1009347.\n\n\nParag, Kris V., Benjamin J. Cowling, and Christl A. Donnelly. 2021.\n“Deciphering Early-Warning Signals of SARS-CoV-2\nElimination and Resurgence from Limited Data at Multiple Scales.”\nJournal of The Royal Society Interface 18 (185): 20210569. https://doi.org/10.1098/rsif.2021.0569.\n\n\nSärkkä, Simo. 2013. Bayesian Filtering and\nSmoothing. 1st ed. Cambridge University Press. https://doi.org/10.1017/CBO9781139344203.\n\n\nSteyn, Nicholas, and Kris V. Parag. 2024. “Robust Uncertainty\nQuantification in Popular Estimators of the Instantaneous Reproduction\nNumber.” medRxiv. https://doi.org/10.1101/2024.10.22.24315918.\n\n\nTemfack, Dhorasso, and Jason Wyse. 2024. “A Review of Sequential\nMonte Carlo Methods for Real-Time Disease Modeling.”\narXiv. https://doi.org/10.48550/ARXIV.2408.15739.\n\n\nThompson, R. N., J. E. Stockwin, R. D. van Gaalen, J. A. Polonsky, Z. N.\nKamvar, P. A. Demarsh, E. Dahlqwist, et al. 2019. “Improved\nInference of Time-Varying Reproduction Numbers During Infectious Disease\nOutbreaks.” Epidemics 29 (December): 100356. https://doi.org/10.1016/j.epidem.2019.100356.\n\n\nYang, Xian, Shuo Wang, Yuting Xing, Ling Li, Richard Yi Da Xu, Karl J.\nFriston, and Yike Guo. 2022. “Bayesian Data Assimilation for\nEstimating Instantaneous Reproduction Numbers During Epidemics:\nApplications to COVID-19.” PLOS\nComputational Biology 18 (2): e1009807. https://doi.org/10.1371/journal.pcbi.1009807.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "notation.html",
    "href": "notation.html",
    "title": "Appendix A — Notation",
    "section": "",
    "text": "Terms are presented in the order they are introduced, except where it makes sense to place similar terms near each other.\n\n\n\n\n\n\n\nTerm\nDefinition\n\n\n\n\nFiltering distribution\nThe posterior distribution of hidden state \\(x_t\\) given observed data until time \\(t\\). Written \\(P(x_t | y_{1:t})\\).\n\n\nSmoothing distribution\nThe posterior distribution of hidden state \\(x_t\\) given all observed data. Written \\(P(x_t|y_{1:T})\\).\n\n\n\n\n\n\nSymbol\nDefinition\n\n\n\n\nIntroduction\n\n\n\n\\(E[\\cdot]\\)\nThe expectation operator.\n\n\n\\(C_t\\)\nReported cases on time-step \\(t\\).\n\n\n\\(I_t\\)\nInfection incidence on time-step \\(t\\).\n\n\n\\(R_t\\)\nThe instantaneous reproduction number at time-step \\(t\\).\n\n\n\\(\\omega_u\\)\nThe serial interval distribution. The probability that a secondary case was reported \\(u\\) days after the primary case.\n\n\n\\(g_u\\)\nThe generation time distribution. The probability that a secondary case was infected \\(u\\) days after the primary case.\n\n\n\\(\\Lambda_t^c\\)\nThe force-of-infection derived from reported cases. Equals \\(\\sum_{u=1}^{u_{max}} C_{t-u} \\omega_u\\).\n\n\n\\(\\Lambda_t\\)\nThe force-of-infection derived from infection incidence. Equals \\(\\sum_{u=1}^{u_{max}} I_{t-u} g_u\\).\n\n\n\\(s:t\\)\nUsed as a subscript, refers to all indices between \\(s\\) and \\(t\\) (inclusive).",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Notation</span>"
    ]
  },
  {
    "objectID": "simulateddata.html",
    "href": "simulateddata.html",
    "title": "Appendix B — Simulated data",
    "section": "",
    "text": "B.1 Simulating the underlying epidemic\nIt is useful to test our models on simulated data where we know what the true value of \\(R_t\\) is. Our simulations are broken into two stages:\nUnsurprisingly, these correspond to the state-space model and observation model in the SMC algorithm.\nAll our simulations leverage the Poisson renewal model:\n\\[ I_t \\sim \\text{Poisson}(R_t \\Lambda_t) \\]\nwhere the force-of-infection is defined as:\n\\[ \\Lambda_t = \\sum_{u = 0}^{t-1} I_{t-u} g_u \\]\nfor some generation time distriubtion \\(g_u\\).\nUnless otherwise stated, we assume a Gaussian random walk for \\(\\log R_t\\):\n\\[ \\log R_t \\sim \\text{Normal}(\\log R_{t-1}, \\sigma) \\]\nwhere \\(\\sigma\\) determines how quickly \\(R_t\\) varies. This matches the state-space model assumed by our default methods.\nSimulations are initialised with \\(I_1\\) infections (default 10) and an initial value of \\(R_1\\) (default 2). We then iteratively sample \\(R_t\\) and \\(I_t\\) from the above model for \\(t = 2, ..., T\\) (default \\(T = 100\\)). The core script looks like:\nAlthough additional code is included to ensure that total infections are between 100 and 100,000 (by default) and that \\(R_t\\) is between 0.2 and 5 (by default). The resulting function simulateSimpleEpidemic() is provided in /src/Simulations.jl. We call it here:\n# using Random, Plots\n# Random.seed!(42)\n\n# include(\"../src/Simulations.jl\")\n\n# Y = simulateSimpleEpidemic()\n\n# plot(plot(Y.Rt, ylabel=\"Reproduction number\", label=false), plot(Y.It, ylabel=\"Infections\", label=false), layout=(2,1))\nThis is also the script that was used to generate “simulated_simple.csv”. We save it instead of calling it each time as the Random seed is not consistent between environments.",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simulated data</span>"
    ]
  },
  {
    "objectID": "simulateddata.html#simulating-the-underlying-epidemic",
    "href": "simulateddata.html#simulating-the-underlying-epidemic",
    "title": "Appendix B — Simulated data",
    "section": "",
    "text": "# Initialise vectors\nlogRt = zeros(T)\nlogRt[1] = log(R1)\nIt = zeros(T)\nIt[1] = log(I1)\n\nfor tt = 2:T\n    # Sample logRt from a normal distribution\n    logRt[tt] = rand(Normal(logRt[tt], σ))\n\n    # Calculate the force of infection\n    Λt = sum(It[tt-1:-1:1] .* g[1:tt-1]) \n\n    # Sample the number of new infections\n    It[tt] = rand(Poisson(exp(logRt[tt]) * Λt))\nend\n\n# Combine into a single dataframe\nY = DataFrame(t=1:T, It=It, Rt=Rt)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simulated data</span>"
    ]
  },
  {
    "objectID": "simulateddata.html#simulating-the-observation-process",
    "href": "simulateddata.html#simulating-the-observation-process",
    "title": "Appendix B — Simulated data",
    "section": "B.2 Simulating the observation process",
    "text": "B.2 Simulating the observation process\nIf we are simulating the simple model then we set \\(C_t = I_t\\) (reported cases = infections) and we are done. Or we can simulate from any number of observation processes. We outline some here.\nAll functions created here are included in src/Simulations.jl.\n\nB.2.1 Binomial reporting\nIn the simple underreporting case, we might have: \\[ C_t \\sim \\text{Binomial}(I_t, \\rho) \\]\nfor some underreporting fraction \\(\\rho\\). We can implement this as:\n\nusing Distributions\n\n# function simulateBinomialReporting(It, ρ)\n\n#     Ct = rand.(Binomial.(It, ρ))\n#     return(Ct)\n\n# end\n\n# Y.Ct = simulateBinomialReporting(Y.It, 0.5)\n\n# plot(Y.It, ylabel=\"Infections\", label=\"True infections\", title=\"Binomial reporting example\", size=(800,400), dpi=300)\n# plot!(Y.Ct, label=\"Reported cases\")\n\n\n\nB.2.2 Delayed reporting\nOr we might assume that cases are underreported and delayed: \\[ C_t \\sim \\text{Binomial}\\left(\\sum_{u=0}^{u_{max}} I_{t-u} d_u, \\ \\rho\\right) \\]\nIf we assume \\(\\rho = 80\\%\\) of cases are reported, and the delay distribution is negative binomial with mean 5.6 and standard deviation 3.2, we can implement:\n\n# function simulateBinomialUnderreportNegBinDelay(It, ρ, r, p)\n\n#     d = pdf.(NegativeBinomial(r, p), 0:length(It))\n#     DelayedIt = round.([sum(It[tt:-1:1] .* d[1:tt]) for tt = 1:length(It)])\n#     Ct = rand.(Binomial.(DelayedIt, ρ))\n#     return(Ct)\n\n# end\n\n# Y.Ct = simulateBinomialUnderreportNegBinDelay(Y.It, 0.8, 7, 5/9)\n\n# plot(Y.It, ylabel=\"Infections\", label=\"True infections\", title=\"Binomial reporting and negative binomial delay\", size=(800,400), dpi=300)\n# plot!(Y.Ct, label=\"Reported cases\")",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simulated data</span>"
    ]
  },
  {
    "objectID": "simulateddata.html#extensions",
    "href": "simulateddata.html#extensions",
    "title": "Appendix B — Simulated data",
    "section": "B.3 Extensions",
    "text": "B.3 Extensions\n\nB.3.1 Including imported cases",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Simulated data</span>"
    ]
  },
  {
    "objectID": "structure.html",
    "href": "structure.html",
    "title": "Appendix D — Structure of pre-built functions",
    "section": "",
    "text": "D.1 Primary function\nThe provided code is built around a user-written function, denoted here bootstrapFilter(θ, Y, opts) . This function should encode the user’s entire state-space model by implementing a bootstrap filter, including initialisation and the projection-weighting-resampling steps.\nThis function should accept:\nand should return a tuple of the form (X, W), where:",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Structure of pre-built functions</span>"
    ]
  },
  {
    "objectID": "structure.html#primary-function",
    "href": "structure.html#primary-function",
    "title": "Appendix D — Structure of pre-built functions",
    "section": "",
    "text": "A parameter vector θ\nSorted dataframe of data Y\nOptions dictionary opts\n\n\n\nX contains the particle values (typically in the form of an \\(N \\times T \\times S\\) vector where \\(S\\) is the number of hidden-states)\nW is a \\(N \\times T\\) matrix of observation weights\n\n\nD.1.1 Example\nOnly minor modifications need to be made to the example in Section 5.1.1 in order to satisfy these requirements:\n\n\nCode\nσ = 0.1 # Model parameters\nnzdata = loadData(\"NZCOVID\") # Dataframe containing model data\nopts = Dict() # A dictionary of parameter values\nopts[\"T\"] = length(nzdata.Ct)\nopts[\"N\"] = 10000\nopts[\"pR0\"] = Uniform(0, 10)\n\nfunction runSimpleModel(σ, nzdata::DataFrame, opts::Dict)\n\n    # Extract frequently used options\n    T = opts[\"T\"]\n    N = opts[\"N\"]\n\n    # Initialise output matrices\n    X = zeros(N, T)\n    W = zeros(N, T)\n\n    # Sample from initial distribution\n    X[:,1] = rand.(opts[\"pR0\"], N)\n\n    # Run the filter\n    for tt = 2:T\n\n        # Project according to the state-space model\n        X[:,tt] = exp.(rand.(Normal.(log.(X[:,tt-1]), σ)))\n\n        # Weight according to the observation model\n        Λ = sum(nzdata.Ct[tt-1:-1:1] .* ω[1:tt-1])\n        W[:,tt] = pdf.(Poisson.(X[:,tt] .* Λ), nzdata.Ct[tt])\n\n        # Resample\n        inds = wsample(1:N, W[:,tt], N; replace=true)\n        X[:, max(tt - L, 1):tt] = X[inds, max(tt - L, 1):tt]\n\n    end\n\n    return(X, W)\n\nend",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Structure of pre-built functions</span>"
    ]
  },
  {
    "objectID": "structure.html#pre-built-functions",
    "href": "structure.html#pre-built-functions",
    "title": "Appendix D — Structure of pre-built functions",
    "section": "D.2 Pre-built functions",
    "text": "D.2 Pre-built functions\nWe provide a collection of functions that accept bootstrapFilter (and also θ, Y, and opts, depending on the function) as an argument:\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nestimateLoglik()\nCalculates and returns the log-likelihood estimate (Equation 5.5). Includes optional argument ignoreerror (default false), that returns \\(-\\infty\\) if the model returns an error.\n\n\nsimplePMMH()\nA simple implementation of the PMMH algorithm. Returns a matrix of accepted values C and a matrix of sampled values with the corresponding likelihood estimate and accept/reject decision OUT.\n\n\nsimplePMMHMulti()\nA multithreaded wrapper of simplePMMH().\n\n\nPMMH()\nA (slightly) more comprehensive and adaptive implementation of the PMMH algorithm. Returns the same arguments, as well as a diagnostics dataframe.\n\n\nPMMHMulti()\nA multithreaded wrapper of PMMH().",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Structure of pre-built functions</span>"
    ]
  }
]