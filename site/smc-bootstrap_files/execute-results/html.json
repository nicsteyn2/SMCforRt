{
  "hash": "6b82f3370ba71b4cd57b21231a733e6a",
  "result": {
    "engine": "jupyter",
    "markdown": "# The bootstrap filter {#sec-smc-bootstrap}\n\n<!-- ```{julia}\n#| echo: false\n#| output: false\n\n# Setting up\nusing Plots, Measures, Distributions\ninclude(\"../src/loadData.jl\")\n``` -->\n\n\nIn this chapter we introduce a variant of the **bootstrap filter**.\n\nThe bootstrap filter generates sample-based approximations to the conditional filtering distribution $P(X_t | y_{1:t}, \\theta)$ and conditional smoothing distribution $P(X_t | y_{1:T}, \\theta)$.\n\n*This chapter assumes fixed and known values of the model parameters $\\theta$. @sec-smc-parameterestimation handles the estimation of $\\theta$.*\n\n\n## Intuition\n\nThe goal of the bootstrap filter is to use observed data $y_{1:T}$ to learn about the hidden-states $X_t$.\n\n::: {.callout-tip}\nWe provide a different intuitive explanation in the corresponding [journal article](https://github.com/nicsteyn2/SMCforRt/blob/main/smcforRt.pdf). Otherwise see @sec-smc-bootstrap-additional for additional resources.\n:::\n\n### Sequential Importance (Re)sampling\n\nAssume that we have a collection of samples from the *filtering distribution* at time $t\\!-\\!1$, denoted $\\{x_{t-1}^{(i)}\\} \\sim P(X_{t-1}|y_{1:t-1}, \\theta)$. We call these samples **particles**, and they collectively represent our knowledge about the hidden-states at this time step.\n\nBy coupling these particles with the state-space transition model (@eq-intro-statespace), we can make one-step-ahead predictions. Mathematically we want to obtain the predictive distribution:\n$$\n\\underbrace{P(X_t | y_{1:t-1}, \\theta)}_{\\text{predictive dist.}} = \\int \\underbrace{P(X_t | X_{t-1}, \\theta)}_{\\text{state-space transition dist.}} \\underbrace{P(X_{t-1}|y_{1:t-1}, \\theta)}_{\\text{prev. filtering dist.}} \\ dX_{t-1}\n$$\nWhich is a complicated and potentially high-dimensional integral. Fortunately, all we need to do is sample from the state-space transition model while conditioning on our particles:\n$$\n\\{\\tilde{x}_t^{(i)}\\} \\sim P(X_t | x_{t-1}^{(i)}, \\theta)\n$$\n\nIf we treat this predictive distribution as the prior distribution for $X_t|y_{1:t}$, we can apply Bayes' formula:\n\n$$\n\\underbrace{P(X_t | y_{1:t}, \\theta)}_{\\text{new filtering dist}} \\propto \\underbrace{P(y_t|X_t, y_{1:t-1}, \\theta)}_{\\text{observation dist.}} \\underbrace{P(X_t | y_{1:t-1}, \\theta)}_{\\text{predictive dist.}}\n$$\n\nSince we already have samples from $P(X_t | y_{1:t-1}, \\theta)$, all we need to do is assign them weights $w_t^{(i)}$ according to the observation distribution:\n\n$$\nw_t^{(i)} = P(y_t | \\tilde{x}_t^{(i)}, y_{1:t-1}, \\theta)\n$$\n\nThe final set of particles $\\{x_t^{(i)}\\}$ is constructed by re-sampling (with replacement) from $\\{\\tilde{x}_t^{(i)}\\}$ according to the corresponding weights. Thus:\n\n$$\nx_t^{(i)} \\sim P(X_t | y_{1:t}, \\theta)\n$$\n\nThus, starting with an initial set of particles $\\{x_0^{(i)}\\} \\sim P(X_0)$, all we need to do is repeat this procedue at each time step $t$, storing the particle values as we go. If the resampling-step is performed at every time step where data are observed, this is called a **bootstrap filter**, which is a special case of a sequential importance resampling algorithm.\n\n### State-history resampling\n\nThe derivation provided above assumes that our model is Markovian: the state-space transition and observation distributions depend only on $X_{t-1}$, and not on $X_{t-2}, X_{t-3}, \\ldots$.\n\n\n\n### Using the particles\n\n## Algorithm\n\n\\#TODO: write SMC algorithm\n\n<!--\n## Example {#sec-smc-bootstrapexample}\n\nFor demonstration, we consider a simple reproduction number estimator. First assume that $\\log R_t$ follows a Gaussian random walk:\n\n$$\\log R_t \\sim \\text{Normal}(\\log R_{t-1}, \\sigma) $$\n\nwhile reported cases are assumed to follow the Poisson renewal model:\n\n$$ C_t \\sim \\text{Poisson}\\left(R_t \\sum_{u=1}^{t-1} C_{t-u} g_u\\right) $$\n\nThe first equation defines our *hidden-state model* while the second equation defines our *observation model*. With only a slight difference[^1], this model is almost identical to that employed by EpiFilter [@paragImprovedEstimationTimevarying2021].\n\n[^1]: @paragImprovedEstimationTimevarying2021 assumes $R_t$ (rather than $\\log R_t$) follows a Gaussian random walk. The standard deviation of this random walk is multiplied by $\\sqrt{R_t}$ to allow $R_t$ to take larger \"jumps\" when it is larger, achieving the same outcome as our log-model. We discuss this further in @sec-other-epifilter.\n\nLeaving parameter estimation to @sec-smc-parameterestimation, we use the following defaults:\n\n::: {#e3a34a72 .cell execution_count=1}\n``` {.julia .cell-code}\n# Serial interval\nω = pdf.(Gamma(2.36, 2.74), 1:100)\nω = ω/sum(ω)\n\n# Smoothing parameter\nσ = 0.15\n\n# Initial distribution for Rt\npR0 = Uniform(0, 10) \n```\n:::\n\n\nCollectively, $\\sigma$, $\\{\\omega_u\\}_{u=1}^{u_{max}}$, and $P(R_0)$ constitute the model parameters $\\theta$.\n\n\n### Data {#sec-smc-data}\n\nWe use data from the first 100 days of the COVID-19 pandemic in New Zealand. Focusing now on total cases (we leave the critical separation of imported and local cases to @sec-models-imported):\n\n::: {#cell-fig-models-nzdata .cell execution_count=2}\n``` {.julia .cell-code code-fold=\"true\"}\nnzdata = loadData(\"NZCOVID\")\nT = length(nzdata.Ct)\nbar(nzdata.date, nzdata.Ct, label=false, xlabel=\"Date\", ylabel=\"Reported cases\", size=(800,300), margins=3mm, color=:darkblue)\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n![Reported cases from the first 100 days of the COVID-19 pandemic in Aotearoa New Zealand.](smc-bootstrap_files/figure-html/fig-models-nzdata-output-1.svg){#fig-models-nzdata}\n:::\n:::\n\n\n### Setting up\n\nWe need to specify the number of particles $N$ and resampling window $L$:\n\n::: {#61f51070 .cell execution_count=3}\n``` {.julia .cell-code}\nN = 10000\nL = 50\n```\n:::\n\n\npre-allocate memory for our particles:\n\n::: {#81f5a1f4 .cell execution_count=4}\n``` {.julia .cell-code}\nX = zeros(N, T)\n```\n:::\n\n\nand sample from the initial distribution:\n\n::: {#8bfc29e4 .cell execution_count=5}\n``` {.julia .cell-code}\nX[:,1] = rand(pR0, N)\n```\n:::\n\n\n### Implementation\n\nAll that's left to do is run the bootstrap filter:\n\n::: {#7b297721 .cell execution_count=6}\n``` {.julia .cell-code}\nfor tt = 2:T\n\n    # Project according to the state-space model\n    X[:,tt] = exp.(rand.(Normal.(log.(X[:,tt-1]), σ)))\n\n    # Weight according to the observation model\n    Λ = sum(nzdata.Ct[tt-1:-1:1] .* ω[1:tt-1])\n    W = pdf.(Poisson.(X[:,tt] .* Λ), nzdata.Ct[tt])\n\n    # Resample\n    inds = wsample(1:N, W, N; replace=true)\n    X[:, max(tt - L, 1):tt] = X[inds, max(tt - L, 1):tt]\n\nend\n```\n:::\n\n\n### Results\n\nThe $t^{th}$ column of $X$ is a set of samples from $P(X_t | C_{1:t+L})$. The mean and quantiles of this posterior distribution are found using:\n\n::: {#cell-fig-models-simpleconditionalsmooth .cell execution_count=7}\n``` {.julia .cell-code}\nm = [mean(X[:,tt]) for tt in 1:T]\nl = [quantile(X[:,tt], 0.025) for tt in 1:T]\nu = [quantile(X[:,tt], 0.975) for tt in 1:T]\nplot(m, ribbon=(m-l, u-m), color=:darkgreen, label=false, xlabel=\"Date\", ylabel=\"Reproduction number\", size=(800,300), margins=3mm)\nhline!([1], label=false, color=:black, line=:dash)\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n![Estimated $R_t$ for the first 100 days of the COVID-19 pandemic. The dashed horizontal line indicates $R_t = 1$. The green line shows the posterior mean and the green shading shows the 95% credible interval.](smc-bootstrap_files/figure-html/fig-models-simpleconditionalsmooth-output-1.svg){#fig-models-simpleconditionalsmooth}\n:::\n:::\n\n\n-->\n\n## Resources from other fields {#sec-smc-bootstrap-additional}\n\nBootstrap filters (and SMC methods more generally) have found use in many fields. Each field has their own motivation for and notation describing these methods. We provide an overview of other resources here.\n\n**Bayesian Filtering and Smoothing [@sarkkaBayesianFilteringSmoothing2013]**\n\nThose with an **engineering background** may be familiar with \"filtering and smoothing\", where the state of a time-varying system is tracked through the observation of noisy measurements. Classical examples include GPS position tracking or audio signal processing.\n\nThe Kalman filter, which provides an analytical solution when the state-space transition and observation models are linear Gaussian and Markovian, is perhaps the best-known example of a filtering method from engineering.\n\nChapters 7 and 11 of @sarkkaBayesianFilteringSmoothing2013 introduce SMC methods under the headings \"particle filtering\" and \"particle smoothing\". We also recommend chapters 1 (*What are Bayesian filtering and smoothing?*), 4 (*Bayesian filtering equations and exact solutions*), and 8 (*Bayesian smoothing equations and exact solutions*).\n\n**A survey of Sequential Monte Carlo Methods for Economics and Finance [@crealSurveySequentialMonte2012]**\n\nThose with an **econometrics background** may find this extensive review helpful, although the author focusses on Markovian models. Examples employed in this review include a stochastic volatility model and a nonlinear dynamic stochastic general equilibrium model.\n\n*This list is incomplete. If you know of any additional resources that may be helpful, [please get in touch](mailto:nicholas.steyn@univ.ox.ac.uk)!*\n\n**Data Assimilation Fundamentals: ... [@evensenDataAssimilationFundamentals2022]**\n\nThose with a background in **atmospheric science, oceanography, metereology, or other environmental sciences** may be familiar with \"data assimilation\", where focus is placed on combining model predictions with observational data. Chapter 9 of this book introduces particle filters as a method for solving the \"fully nonlinear data assimilation\" problem.\n\n",
    "supporting": [
      "smc-bootstrap_files"
    ],
    "filters": [],
    "includes": {}
  }
}